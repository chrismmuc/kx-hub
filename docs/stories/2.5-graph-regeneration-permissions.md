# Story 2.5: Graph Regeneration & Storage Permissions

## Status: PLANNED
**Epic:** 2 - AI-Powered Knowledge Processing
**Dependencies:** Story 2.3 (Clustering Consistency)
**Priority:** MEDIUM (Operational improvement)

---

## Problem Statement

### Issue 1: Graph Not Regenerated After Delta Clustering

**Current behavior:**
```python
# functions/clustering/main.py:345-346
# TODO: Optionally regenerate graph.json with updated memberships
# This could be expensive for large datasets, so might be done separately
```

**Impact:**
- `graph.json` becomes stale after new chunks added via delta clustering
- Visualization doesn't reflect latest cluster memberships
- Manual regeneration required

### Issue 2: Storage Permission Denied

**Error during initial load:**
```
google.api_core.exceptions.Forbidden: 403 POST https://storage.googleapis.com/upload/storage/v1/b/kx-hub-pipeline/o
mcp-server-sa@kx-hub.iam.gserviceaccount.com does not have storage.objects.create access
```

**Impact:**
- Cannot upload `graph.json` to `gs://kx-hub-pipeline/graphs/`
- Service account lacks `storage.objects.create` permission
- Graph generation fails silently

---

## Solution Overview

### Part A: Enable Graph Regeneration

Add graph regeneration to delta clustering Cloud Function with smart caching:

1. **Efficient regeneration**: Only regenerate if cluster membership changed significantly
2. **Async generation**: Generate graph asynchronously to avoid blocking delta processing
3. **Incremental updates**: Update graph nodes instead of full regeneration (future enhancement)

### Part B: Fix Storage Permissions

Grant proper IAM permissions to service account:

```bash
# Grant storage object creator role
gcloud projects add-iam-policy-binding kx-hub \
    --member="serviceAccount:mcp-server-sa@kx-hub.iam.gserviceaccount.com" \
    --role="roles/storage.objectCreator"
```

---

## Technical Implementation

### A1: Smart Graph Regeneration Logic

**Decision criteria for regeneration:**

```python
def should_regenerate_graph(
    num_new_chunks: int,
    total_chunks: int,
    threshold: float = 0.01  # 1% change
) -> bool:
    """
    Determine if graph should be regenerated.

    Args:
        num_new_chunks: Number of chunks added in this delta run
        total_chunks: Total number of chunks in system
        threshold: Percentage change threshold for regeneration

    Returns:
        True if graph should be regenerated
    """
    change_ratio = num_new_chunks / total_chunks
    return change_ratio >= threshold
```

**Examples:**
- 5 new chunks / 823 total = 0.6% → Skip regeneration ❌
- 10 new chunks / 823 total = 1.2% → Regenerate ✅
- 50 new chunks / 823 total = 6% → Regenerate ✅

### A2: Async Graph Generation (Future)

For large datasets, generate graph asynchronously:

```python
# Option 1: Cloud Tasks (recommended)
from google.cloud import tasks_v2

task = {
    'http_request': {
        'http_method': 'POST',
        'url': 'https://graph-generator-function.run.app',
        'body': json.dumps({'trigger': 'delta_clustering'})
    }
}
tasks_client.create_task(parent=queue_path, task=task)
```

**Benefits:**
- Delta clustering returns immediately
- Graph generation happens in background
- User doesn't wait for expensive graph computation

### A3: Implementation in Cloud Function

```python
# functions/clustering/main.py (after Step 4)

# Step 5: Conditionally regenerate graph
if should_regenerate_graph(len(valid_chunk_ids), total_chunk_count):
    logger.info("[Step 5/5] Regenerating graph.json...")

    try:
        # Load all chunks with embeddings (needed for graph)
        all_chunks, all_embeddings, _ = load_all_chunks_for_graph(db)

        # Load cluster labels
        cluster_labels = get_cluster_labels_for_chunks(db, all_chunks)

        # Generate graph
        from clustering.graph_generator import GraphGenerator
        generator = GraphGenerator(
            similarity_threshold=0.7,
            max_edges_per_node=5
        )
        graph = generator.generate(all_chunks, all_embeddings, cluster_labels)

        # Upload to Cloud Storage
        upload_graph_to_storage(storage_client, bucket_name, graph)

        logger.info("✅ Graph regenerated successfully")

    except Exception as e:
        # Non-blocking: Log error but don't fail clustering
        logger.warning(f"Graph regeneration failed (non-critical): {e}")
else:
    logger.info(f"Skipping graph regeneration ({len(valid_chunk_ids)} new chunks < 1% threshold)")
```

---

## Storage Permissions Fix

### Current IAM Setup

Check current permissions:
```bash
gcloud projects get-iam-policy kx-hub \
    --flatten="bindings[].members" \
    --filter="bindings.members:mcp-server-sa@kx-hub.iam.gserviceaccount.com"
```

**Likely current roles:**
- `roles/datastore.user` (Firestore access)
- `roles/storage.objectViewer` (read-only storage)

**Missing:**
- `roles/storage.objectCreator` (write access)

### Solution: Grant Object Creator Role

**Option A: Project-level (broad access)**
```bash
gcloud projects add-iam-policy-binding kx-hub \
    --member="serviceAccount:mcp-server-sa@kx-hub.iam.gserviceaccount.com" \
    --role="roles/storage.objectCreator"
```

**Option B: Bucket-level (recommended, least privilege)**
```bash
gcloud storage buckets add-iam-policy-binding gs://kx-hub-pipeline \
    --member="serviceAccount:mcp-server-sa@kx-hub.iam.gserviceaccount.com" \
    --role="roles/storage.objectCreator"
```

**Permissions granted:**
- `storage.objects.create`
- `storage.objects.delete` (optional, for overwriting)

### Terraform Implementation

Update `terraform/iam.tf`:

```hcl
# Grant storage write access to MCP service account
resource "google_storage_bucket_iam_member" "mcp_sa_object_creator" {
  bucket = google_storage_bucket.pipeline.name
  role   = "roles/storage.objectCreator"
  member = "serviceAccount:${google_service_account.mcp_server.email}"
}
```

---

## Performance Considerations

### Graph Generation Performance

**Current metrics (823 chunks, 37 clusters):**
- Load all chunks: ~2 seconds
- Compute similarity edges: ~1 second
- Upload to Cloud Storage: ~0.5 seconds
- **Total: ~3.5 seconds**

**With 10,000 chunks:**
- Load all chunks: ~20 seconds
- Compute similarity edges: ~10 seconds
- **Total: ~30 seconds** ⚠️ Too slow for sync

**Threshold for async:**
- < 2,000 chunks: Synchronous ✅
- \> 2,000 chunks: Asynchronous (Cloud Tasks) ✅

### Delta Processing Impact

**Before (no graph regeneration):**
- Delta clustering: ~7 seconds

**After (with smart regeneration):**
- Most runs: ~7 seconds (skip regeneration)
- 1% threshold runs: ~10.5 seconds (3.5s for graph)
- Still within target < 15 seconds ✅

---

## Testing

### Test 1: Storage Permission Verification

```bash
# Test write access
echo '{"test": true}' | gcloud storage cp - gs://kx-hub-pipeline/test.json

# Clean up
gcloud storage rm gs://kx-hub-pipeline/test.json
```

**Expected:** File uploaded successfully

### Test 2: Graph Regeneration Logic

```python
def test_should_regenerate_graph():
    """Verify graph regeneration threshold logic."""
    # Below threshold
    assert not should_regenerate_graph(5, 823, threshold=0.01)

    # At threshold
    assert should_regenerate_graph(9, 823, threshold=0.01)

    # Above threshold
    assert should_regenerate_graph(50, 823, threshold=0.01)
```

### Test 3: End-to-End Graph Upload

```python
def test_graph_upload_after_delta():
    """Verify graph is uploaded after delta clustering."""
    # Trigger delta clustering with 10+ chunks (above threshold)
    response = requests.post(
        f"{cloud_function_url}/cluster_new_chunks",
        json={"chunk_ids": [f"chunk-{i}" for i in range(10)]}
    )

    assert response.status_code == 200

    # Verify graph.json was updated
    bucket = storage_client.bucket('kx-hub-pipeline')
    blob = bucket.blob('graphs/graph.json')

    assert blob.exists()

    # Check updated timestamp
    updated_time = blob.updated
    assert updated_time > datetime.now() - timedelta(minutes=1)
```

---

## Migration Steps

### Step 1: Fix Storage Permissions

```bash
# 1. Verify current permissions
gcloud storage buckets get-iam-policy gs://kx-hub-pipeline

# 2. Grant object creator role (bucket-level)
gcloud storage buckets add-iam-policy-binding gs://kx-hub-pipeline \
    --member="serviceAccount:mcp-server-sa@kx-hub.iam.gserviceaccount.com" \
    --role="roles/storage.objectCreator"

# 3. Verify permission granted
gcloud storage buckets get-iam-policy gs://kx-hub-pipeline | \
    grep -A 5 "mcp-server-sa"
```

### Step 2: Test Permission Fix

```bash
# Run initial clustering to verify upload works
export GCP_PROJECT=kx-hub
export GCP_REGION=europe-west4
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json

python3 -m src.clustering.initial_load
```

**Expected output:**
```
✅ Graph uploaded to gs://kx-hub-pipeline/graphs/graph.json
```

### Step 3: Update Cloud Function Code

```python
# functions/clustering/main.py
# Add Step 5: Graph regeneration after Step 4: Update Firestore
```

### Step 4: Deploy Updated Cloud Function

```bash
cd terraform
terraform apply
```

### Step 5: Verify in Production

Trigger delta clustering with 10+ chunks and verify graph is regenerated.

---

## Configuration

### Environment Variables

Add to Cloud Function configuration:

```yaml
# terraform/clustering.tf
environment_variables = {
  GCP_PROJECT            = var.project_id
  FIRESTORE_COLLECTION   = "kb_items"
  GCS_BUCKET             = "${var.project_id}-pipeline"
  GRAPH_REGEN_THRESHOLD  = "0.01"  # 1% change triggers regeneration
  GRAPH_MAX_SYNC_CHUNKS  = "2000"  # Above this, use async
}
```

### Feature Flags

Make graph regeneration configurable:

```python
ENABLE_GRAPH_REGEN = os.getenv('ENABLE_GRAPH_REGEN', 'true').lower() == 'true'

if ENABLE_GRAPH_REGEN and should_regenerate_graph(...):
    # Regenerate graph
    ...
```

---

## Cost Impact

### Storage Costs

**Graph file size:**
- 823 chunks: ~500KB JSON
- 10,000 chunks: ~5MB JSON

**Cloud Storage costs:**
- Storage: $0.02/GB/month
- Upload operations: $0.005/10K operations
- **Impact:** < $0.01/month

### Cloud Function Costs

**Additional compute:**
- Graph regeneration: ~3.5 seconds (sync) or background (async)
- Frequency: ~1-2 times per week (1% threshold)
- **Impact:** < $0.01/month

**Total additional cost:** < $0.02/month (negligible)

---

## Rollback Plan

If graph regeneration causes issues:

1. **Disable via environment variable:**
   ```bash
   gcloud functions update clustering-function \
       --update-env-vars ENABLE_GRAPH_REGEN=false
   ```

2. **Revert Cloud Function:**
   ```bash
   cd terraform
   git revert <commit-hash>
   terraform apply
   ```

3. **Manual graph generation:**
   ```bash
   python3 -m src.clustering.generate_graph
   ```

---

## Acceptance Criteria

- [ ] Storage permissions granted to `mcp-server-sa` service account
- [ ] Initial clustering successfully uploads `graph.json`
- [ ] Cloud Function includes graph regeneration logic
- [ ] Smart threshold prevents unnecessary regeneration (< 1% change)
- [ ] Graph regeneration completes in < 5 seconds for current dataset
- [ ] Non-blocking error handling (graph failure doesn't fail clustering)
- [ ] Environment variable for enabling/disabling feature
- [ ] Integration tests verify graph upload
- [ ] Monitoring alerts for repeated graph generation failures

---

## Future Enhancements

### 1. Incremental Graph Updates

Instead of full regeneration, update only changed nodes:

```python
def update_graph_incrementally(
    existing_graph: Dict,
    new_chunks: List[Dict],
    new_labels: np.ndarray
) -> Dict:
    """
    Update graph by adding new nodes, not regenerating everything.

    Args:
        existing_graph: Current graph from storage
        new_chunks: Newly added chunks
        new_labels: Cluster assignments for new chunks

    Returns:
        Updated graph
    """
    # Add new nodes
    for chunk, label in zip(new_chunks, new_labels):
        existing_graph['nodes'].append({
            'id': chunk['id'],
            'cluster_id': f'cluster-{label}',
            'title': chunk['title']
        })

    # Add edges from new nodes to existing nodes
    # (only compute edges for new nodes, not all nodes)

    return existing_graph
```

**Benefits:**
- Much faster (no need to reload all chunks)
- Scales to large datasets
- Still maintains graph consistency

### 2. Graph Versioning

Store multiple graph versions:

```
gs://kx-hub-pipeline/graphs/
  ├── graph-latest.json (symlink)
  ├── graph-2025-11-02.json
  ├── graph-2025-11-01.json
  └── graph-2025-10-31.json
```

**Benefits:**
- Rollback capability
- A/B testing different graph algorithms
- Historical analysis

### 3. Async Graph Generation with Cloud Tasks

For large datasets (> 2,000 chunks):

```python
if total_chunks > 2000:
    # Create Cloud Task for async generation
    task_client = tasks_v2.CloudTasksClient()
    task = {
        'http_request': {
            'http_method': 'POST',
            'url': 'https://graph-generator-function.run.app',
            'body': json.dumps({'chunks': chunk_ids})
        }
    }
    task_client.create_task(parent=queue_path, task=task)
    logger.info("Scheduled async graph generation (large dataset)")
else:
    # Sync generation for small datasets
    generate_graph_sync()
```

---

## References

- [Cloud Storage IAM Roles](https://cloud.google.com/storage/docs/access-control/iam-roles)
- [Cloud Tasks Documentation](https://cloud.google.com/tasks/docs)
- [Graph Algorithms for Large Datasets](https://arxiv.org/abs/2103.00905)

---

**Story 2.5 prepared for implementation**
**Addresses both graph regeneration and storage permissions**
