---
epicNum: 1
storyNum: 7
title: "MCP Server for Conversational Knowledge Base Access"
status: "Ready"
---

# Story: MCP Server for Conversational Knowledge Base Access

As a **knowledge base user**, I want **to query my highlights and documents conversationally through Claude Desktop using natural language**, so that **I can access my knowledge without context switching, manually searching, or copy-pasting content into conversations**.

## Problem Statement

Currently, the kx-hub system has 813 chunks with semantic embeddings stored in Firestore, but there's no user-facing interface to query this knowledge. The original PRD anticipated building a traditional REST API + CLI/Web UI (Use Case #7: "Query-Driven Retrieval"). However, this approach has several limitations:

1. **High Implementation Cost**: Building a REST API + UI client requires 7-10 days of development effort
2. **Context Switching**: Users must switch between their AI assistant (Claude) and a separate search interface
3. **Manual Copy-Paste**: Search results must be manually copied into Claude conversations
4. **Unnatural Interaction**: Form-based search is less intuitive than conversational queries
5. **Limited Composability**: Results can't be easily combined with AI reasoning workflows

## Acceptance Criteria

1. **MCP Server Implementation**: Implement a local MCP (Model Context Protocol) server that exposes kx-hub's knowledge base to Claude Desktop via stdio transport.

2. **Resource Exposure**: Expose Firestore `kb_items` chunks as MCP resources with URIs:
   ```
   kxhub://chunk/{chunk_id}
   kxhub://chunks/by-source/{source}
   kxhub://chunks/by-author/{author}
   kxhub://chunks/by-tag/{tag}
   ```
   - List all available chunks with metadata
   - Read individual chunk content with full markdown
   - Filter by metadata (source, author, tags)

3. **Search Tools**: Implement MCP tools for semantic and metadata-based search:
   ```python
   search_semantic(query: str, limit: int = 10, filters: dict = {})
     → Returns: List[Chunk] ranked by cosine similarity

   search_by_metadata(tags: List[str] = None, author: str = None, source: str = None, limit: int = 20)
     → Returns: Filtered chunks matching criteria

   get_related_chunks(chunk_id: str, limit: int = 5)
     → Returns: Similar chunks via vector similarity

   get_stats()
     → Returns: Total chunks, sources, tags, authors summary
   ```

4. **Prompt Templates**: Provide pre-defined prompts for common queries:
   ```
   "find_insights_about": "Search my reading highlights for insights about {topic}"
   "author_deep_dive": "Show all highlights from {author} and identify key themes"
   "tag_exploration": "Explore all content tagged with {tag}"
   "related_to_chunk": "Find highlights related to chunk {chunk_id}"
   ```

5. **Claude Desktop Integration**: Configure Claude Desktop to connect to kx-hub MCP server with proper GCP authentication and test end-to-end queries.

6. **Query Embedding**: Generate embeddings for user queries using the same `gemini-embedding-001` model (768 dimensions) to ensure semantic consistency with stored chunks.

7. **Performance**: Search queries complete in <1 second (P95) for typical result sets (10-20 chunks), leveraging Firestore's native vector search.

8. **Documentation**: Provide setup guide, example queries, troubleshooting, and architecture overview for MCP integration.

## Dev Notes

- **Architecture Context**
  - Current state: 813 chunks with 768-dim embeddings in Firestore `kb_items` collection [Source: Story 1.6]
  - Firestore vector index already configured for FIND_NEAREST queries [Source: terraform/main.tf:457-463]
  - Embedding model: `gemini-embedding-001` via Vertex AI with `output_dimensionality=768` [Source: src/embed/main.py:303]
  - PRD Use Case #7: "Query-Driven Retrieval" - MCP provides superior UX vs traditional REST API
  - No breaking changes: MCP server is additive, doesn't modify existing pipeline

- **MCP Protocol Overview**
  - **What is MCP?**: Anthropic's Model Context Protocol - open standard for connecting AI applications to data sources
  - **Transport**: stdio (JSON-RPC over standard input/output) - simple, local, no hosting needed
  - **Primitives**: Resources (read-only data), Tools (executable functions), Prompts (templates), Sampling (LLM requests)
  - **Client Support**: Claude Desktop (official), VS Code, Zed, custom AI applications
  - **Documentation**: https://modelcontextprotocol.io/

- **Why MCP vs Traditional API?**
  - **Faster Time to Value**: 2-3 days vs 7-10 days for REST API + UI
  - **Better UX**: Conversational queries vs form-based search
  - **Zero Context Switching**: Query within Claude conversations
  - **Protocol-First**: Standard interface, works with any MCP client
  - **Zero Cost**: Local server, no hosting/infrastructure needed
  - **Future-Proof**: Anthropic-backed standard with growing ecosystem
  - **Complementary**: Can coexist with future REST API if needed

- **Technical Approach - Local stdio Server**
  - **Location**: Runs on user's local machine (not deployed to GCP)
  - **Transport**: stdio (standard input/output) - simplest MCP transport
  - **Authentication**: GCP service account credentials via environment variable
  - **SDK**: MCP Python SDK (`pip install mcp`)
  - **Architecture**:
    ```
    ┌──────────────┐   stdio    ┌────────────────┐   HTTPS   ┌──────────────┐
    │ Claude App   │◄──────────►│ Local Python   │◄─────────►│ GCP Firestore│
    │ (Desktop)    │  JSON-RPC  │ MCP Server     │   API     │ Vector Search│
    └──────────────┘            └────────────────┘           └──────────────┘
    ```

- **Implementation Structure**
  ```
  src/mcp_server/
  ├── main.py              # MCP server entry point
  ├── resources.py         # Resource handlers (list/read chunks)
  ├── tools.py             # Tool handlers (search, filter, stats)
  ├── prompts.py           # Prompt templates
  ├── firestore_client.py  # Firestore query wrapper
  ├── embeddings.py        # Query embedding generation
  └── requirements.txt     # Dependencies
  ```

- **Resource Implementation**
  - List resources: Query `kb_items` collection, return URI + metadata for each chunk
  - Read resource: Parse URI → extract chunk_id → fetch from Firestore → return markdown content
  - Support filtering: Query by `source`, `author`, `tags` fields
  - Include metadata: title, author, source, tags, chunk_index, total_chunks

- **Tool Implementation - search_semantic**
  1. Generate embedding for query text using `gemini-embedding-001` (768 dimensions)
  2. Execute Firestore FIND_NEAREST query on `kb_items.embedding` field
  3. Apply filters (tags, author, source) if provided
  4. Return top N chunks with metadata + content snippet (first 500 chars)
  5. Include similarity scores for ranking

- **Tool Implementation - search_by_metadata**
  - Query Firestore with WHERE clauses on metadata fields
  - Support multiple tags (array-contains-any)
  - Support author exact match
  - Support source exact match
  - Return chunks sorted by created_at (most recent first)

- **Tool Implementation - get_related_chunks**
  - Fetch chunk by chunk_id
  - Extract its embedding vector
  - Execute FIND_NEAREST with that vector
  - Exclude the original chunk from results
  - Return top N similar chunks

- **Tool Implementation - get_stats**
  - Query Firestore for document counts
  - Aggregate unique sources, authors, tags
  - Return summary statistics (total chunks, avg chunks per doc, etc.)

- **Query Embedding Generation**
  - Reuse existing Vertex AI client code from `src/embed/main.py`
  - Use same model: `gemini-embedding-001`
  - Use same dimensionality: `output_dimensionality=768`
  - Handle retries and backoff (same as document embedding)
  - Cache client initialization for performance

- **Claude Desktop Configuration**
  ```json
  // ~/Library/Application Support/Claude/claude_desktop_config.json
  {
    "mcpServers": {
      "kx-hub": {
        "command": "python3",
        "args": ["/Users/christian/dev/kx-hub/src/mcp_server/main.py"],
        "env": {
          "GOOGLE_APPLICATION_CREDENTIALS": "/path/to/service-account.json",
          "GCP_PROJECT": "kx-hub",
          "GCP_REGION": "europe-west4",
          "FIRESTORE_COLLECTION": "kb_items"
        }
      }
    }
  }
  ```

- **Security Considerations**
  - Uses GCP service account with minimal permissions (Firestore read, Vertex AI predict)
  - No network exposure (stdio transport is local-only)
  - No API keys or secrets in MCP server code (all via env vars)
  - Follows existing GCP IAM patterns from pipeline functions

- **Error Handling**
  - Graceful degradation if Firestore unavailable (return error message to Claude)
  - Retry logic for Vertex AI embedding API (same as existing pipeline)
  - Validate chunk_id format before Firestore queries
  - Handle empty result sets gracefully
  - Log errors to stderr (visible in Claude Desktop MCP logs)

- **Performance Optimizations**
  - Cache Firestore client initialization (reuse across requests)
  - Cache Vertex AI client initialization (reuse across requests)
  - Limit result sets to reasonable defaults (10-20 chunks)
  - Stream large result sets if needed
  - Use Firestore query limits to prevent over-fetching

- **Testing Strategy**
  - Unit tests: Resource handlers, tool functions, embedding generation
  - Integration tests: End-to-end MCP protocol communication (stdio)
  - Manual testing: Real queries in Claude Desktop
  - Test fixtures: Sample chunks, mock Firestore responses
  - Performance testing: Query latency measurement

- **Cost Impact**
  - **No additional infrastructure cost** (local server, no hosting)
  - **Vertex AI embeddings**: Query embeddings only (~$0.00001 per query)
  - **Firestore reads**: ~10-20 reads per query (well within free tier)
  - **Estimated monthly cost**: +$0.10-0.20 for typical usage (50-100 queries/month)
  - **Total kx-hub cost**: $1.40 → $1.50-1.60/month (~7% increase)

- **Future Enhancements** (Out of Scope for 1.7)
  - Remote MCP server (SSE transport via Cloud Function)
  - Multi-user support with authentication
  - Caching layer for frequent queries
  - Query analytics and usage tracking
  - Advanced filters (date ranges, token count, similarity threshold)
  - Batch operations (compare multiple chunks)
  - Export results to markdown files

## Tasks / Subtasks

1. **Setup & Dependencies (AC1)**
   - [ ] Install MCP Python SDK: `pip install mcp`
   - [ ] Create project structure: `src/mcp_server/` directory
   - [ ] Setup requirements.txt with dependencies (mcp, google-cloud-firestore, google-cloud-aiplatform)
   - [ ] Create basic server skeleton (main.py with MCP server initialization)
   - [ ] Setup logging configuration (stderr for MCP protocol compliance)

2. **Firestore Client Integration (AC2)**
   - [ ] Create `firestore_client.py` wrapper module
   - [ ] Implement `list_all_chunks()` - query kb_items collection
   - [ ] Implement `get_chunk_by_id(chunk_id)` - fetch single chunk
   - [ ] Implement `query_by_metadata(filters)` - WHERE clauses on tags/author/source
   - [ ] Implement `find_nearest(embedding, limit, filters)` - vector search wrapper
   - [ ] Add connection pooling and client caching
   - [ ] Handle GCP authentication via service account

3. **Resource Handlers (AC2)**
   - [ ] Implement `list_resources()` handler
     - Query all chunks from Firestore
     - Generate URIs for each chunk (kxhub://chunk/{id})
     - Include metadata (title, author, source, tags)
     - Return MCP Resource objects
   - [ ] Implement `read_resource(uri)` handler
     - Parse URI to extract chunk_id/filter
     - Fetch chunk from Firestore
     - Return full markdown content
     - Handle resource not found errors
   - [ ] Support filtered resource listings (by-source, by-author, by-tag URIs)

4. **Embedding Generation (AC6)**
   - [ ] Create `embeddings.py` module
   - [ ] Initialize Vertex AI client (reuse code from src/embed/main.py)
   - [ ] Implement `generate_query_embedding(text)` function
     - Use gemini-embedding-001 model
     - Set output_dimensionality=768
     - Handle retries and backoff
     - Return 768-dimensional vector
   - [ ] Add client caching to avoid re-initialization
   - [ ] Unit tests for embedding generation

5. **Search Tools Implementation (AC3)**
   - [ ] Create `tools.py` module
   - [ ] Implement `search_semantic(query, limit, filters)` tool
     - Generate embedding for query text
     - Execute Firestore FIND_NEAREST query
     - Apply metadata filters if provided
     - Format results (metadata + content snippet)
     - Return ranked chunks with similarity scores
   - [ ] Implement `search_by_metadata(tags, author, source, limit)` tool
     - Build Firestore WHERE query
     - Support multiple tags (array-contains-any)
     - Handle author/source exact match
     - Return filtered chunks sorted by date
   - [ ] Implement `get_related_chunks(chunk_id, limit)` tool
     - Fetch source chunk embedding
     - Execute vector similarity search
     - Exclude source chunk from results
     - Return top N similar chunks
   - [ ] Implement `get_stats()` tool
     - Query collection counts
     - Aggregate unique authors, sources, tags
     - Return summary statistics

6. **Prompt Templates (AC4)**
   - [ ] Create `prompts.py` module
   - [ ] Define `find_insights_about` prompt template
     - Parameter: {topic}
     - Description: Search highlights for insights about topic
   - [ ] Define `author_deep_dive` prompt template
     - Parameter: {author}
     - Description: Show all highlights from author and identify themes
   - [ ] Define `tag_exploration` prompt template
     - Parameter: {tag}
     - Description: Explore all content tagged with tag
   - [ ] Define `related_to_chunk` prompt template
     - Parameter: {chunk_id}
     - Description: Find highlights related to specific chunk
   - [ ] Register prompts with MCP server

7. **MCP Server Integration (AC1, AC5)**
   - [ ] Wire resource handlers to MCP server
   - [ ] Wire tool handlers to MCP server
   - [ ] Wire prompt templates to MCP server
   - [ ] Implement proper JSON-RPC error responses
   - [ ] Add request/response logging (debug mode)
   - [ ] Handle graceful shutdown
   - [ ] Test stdio transport communication

8. **Claude Desktop Configuration (AC5)**
   - [ ] Document configuration file location
     - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
     - Linux: `~/.config/Claude/claude_desktop_config.json`
   - [ ] Create example configuration JSON
   - [ ] Document required environment variables
     - GOOGLE_APPLICATION_CREDENTIALS path
     - GCP_PROJECT, GCP_REGION
     - FIRESTORE_COLLECTION
   - [ ] Test server startup from Claude Desktop
   - [ ] Verify MCP server appears in Claude's available tools

9. **End-to-End Testing (AC5, AC7)**
   - [ ] Test resource listing in Claude Desktop
   - [ ] Test resource reading (individual chunks)
   - [ ] Test semantic search with various queries
   - [ ] Test metadata filtering (by author, tag, source)
   - [ ] Test related chunks discovery
   - [ ] Test prompt templates
   - [ ] Measure query performance (target <1s P95)
   - [ ] Test error handling (invalid chunk_id, empty results)
   - [ ] Test with 10+ diverse queries to validate relevance

10. **Documentation (AC8)**
    - [ ] Create `docs/mcp-server-setup.md`
      - Installation instructions
      - Claude Desktop configuration
      - GCP authentication setup
      - Troubleshooting common issues
    - [ ] Create `docs/mcp-server-usage.md`
      - Example queries and use cases
      - Tool descriptions and parameters
      - Resource URI patterns
      - Prompt template examples
    - [ ] Create `docs/architecture/mcp-integration.md`
      - Architecture diagram
      - Protocol flow
      - Security model
      - Performance characteristics
    - [ ] Update main README with MCP server section
    - [ ] Add example query scenarios to documentation

11. **Unit Testing**
    - [ ] Create `tests/test_mcp_resources.py`
      - Test list_resources with mock Firestore
      - Test read_resource with various URIs
      - Test resource filtering logic
    - [ ] Create `tests/test_mcp_tools.py`
      - Test search_semantic with mock embeddings + Firestore
      - Test search_by_metadata with various filters
      - Test get_related_chunks
      - Test get_stats
    - [ ] Create `tests/test_mcp_embeddings.py`
      - Test query embedding generation
      - Test client caching
      - Test error handling and retries
    - [ ] Create `tests/test_mcp_server.py`
      - Test MCP protocol message handling
      - Test JSON-RPC request/response format
      - Test error responses

12. **Performance Optimization & Validation (AC7)**
    - [ ] Profile query latency (embedding generation + Firestore query)
    - [ ] Implement client connection pooling if needed
    - [ ] Add query result caching (optional, if needed)
    - [ ] Validate <1s P95 performance target
    - [ ] Document performance characteristics
    - [ ] Add performance monitoring/logging

---

**Dependencies:** Story 1.6 (Intelligent Document Chunking) must be complete with 813 chunks deployed to Firestore with 768-dimensional embeddings.

**Estimated Complexity:** Medium — New MCP server component, but leverages existing Firestore vector search and embedding infrastructure. No changes to existing pipeline.

**Success Metrics:**
- MCP server successfully connects to Claude Desktop
- Semantic search returns relevant results in <1 second (P95)
- All 4 tools (search_semantic, search_by_metadata, get_related_chunks, get_stats) functional
- 10+ test queries demonstrate accurate, relevant results
- Zero infrastructure cost increase (local server)
- User can query knowledge base conversationally without leaving Claude

**Cost Summary:**
- Current: $1.40/month (Story 1.6 deployed)
- MCP Server: +$0.10-0.20/month (query embeddings only)
- Proposed: $1.50-1.60/month total (+7-14% increase)
- **ROI**: Conversational knowledge access + zero context switching for ~$0.15/month

## Dev Agent Record

*This section will be completed during implementation*

## QA Results

*This section will be completed after QA review*
