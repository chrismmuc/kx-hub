# Story 2.2: Semantic Clustering with Initial Load & Delta Processing

Status: done

## Story

As a knowledge base user,
I want my 818 knowledge chunks automatically grouped into semantic clusters based on topic similarity,
so that I can discover thematic patterns, explore related ideas, and navigate my knowledge graph by topic rather than search keywords alone.

**Two Execution Modes:**
1. **Initial Load (One-Time):** Local Python script to bulk cluster all 818 existing chunks and directly update Firestore
2. **Delta Processing (Ongoing):** Cloud Function automatically assigns new chunks to existing clusters (or creates new clusters) as part of the daily batch pipeline

## Acceptance Criteria

1. **100% coverage (Initial Load):** All 818 chunks in Firestore `kb_items` collection assigned to at least one cluster_id
2. **Cluster quality:** ≥80% of cluster members are semantically related based on manual spot-check of 20 random clusters
3. **Idempotency:** Initial load script can be re-run to recompute clusters without data corruption
4. **Delta processing integration:** Cloud Function automatically processes new chunks after Knowledge Cards step in pipeline
5. **Performance (Initial Load):** Completes clustering of 818 chunks in ≤10 minutes
6. **Performance (Delta Processing):** Processes batch of new chunks in ≤5 minutes
7. **Data persistence:** Cluster assignments stored in `kb_items.cluster_id` field (array of cluster IDs)
8. **Graph export:** Generate `graph.json` in Cloud Storage with cluster relationships and metadata
9. **Cost efficiency:** Total cost ≤$0.10/month (reuses existing embeddings, no new AI calls)
10. **Firestore batch efficiency:** Use batch writes (≤500 ops/batch) for optimal performance

## Tasks / Subtasks

- [x] Task 1: Research and select clustering algorithm (AC: #2, #10)
  - [x] Evaluate K-Means vs HDBSCAN vs Hierarchical clustering
  - [x] Test with sample of 100 chunks using existing 768-dim embeddings
  - [x] Measure cluster quality (silhouette score, manual inspection)
  - [x] Document algorithm selection rationale with web research findings
  - [x] Validate cosine similarity metric for semantic clustering

- [x] Task 2: Design cluster storage schema (AC: #7, #8)
  - [x] Define `kb_items.cluster_id` field schema (array of strings)
  - [x] Design optional `kb_clusters` collection schema (id, label, member_count, created_at)
  - [x] Define `graph.json` structure for Epic 3 export (nodes, edges, cluster metadata)
  - [x] Document schema in code comments and architecture docs

- [x] Task 3: Implement core clustering logic module (AC: #2, #5, #6)
  - [x] Create `src/clustering/clusterer.py` with reusable clustering logic
  - [x] Implement cosine similarity distance calculation
  - [x] Implement selected clustering algorithm (HDBSCAN or K-Means)
  - [x] Add cluster quality metrics (silhouette score, inertia if K-Means)
  - [x] Handle edge cases: single-member clusters, noise points (-1 label for HDBSCAN)

- [x] Task 4: Implement initial load script (AC: #1, #3, #5, #10)
  - [x] Load all 818 chunks with embeddings from Firestore `kb_items`
  - [x] Run clustering algorithm on all embeddings
  - [x] Batch update Firestore with cluster_id assignments (500 updates/batch)
  - [x] Log progress every 100 chunks
  - [x] Generate graph.json and upload to Cloud Storage
  - [x] Implement idempotency: clear existing cluster_id arrays before recomputing
  - [x] Handle retry logic for Firestore batch write failures

- [x] Task 5: Implement delta processing Cloud Function (AC: #4, #6, #10)
  - [x] Create `functions/clustering/main.py` Cloud Function entry point
  - [x] Accept batch of new chunk IDs via HTTP POST from Cloud Workflows
  - [x] Load existing cluster centroids/representatives from Firestore
  - [x] For each new chunk: assign to nearest existing cluster OR create new cluster
  - [x] Batch update Firestore with new cluster assignments (500 updates/batch)
  - [x] Regenerate graph.json with updated cluster memberships
  - [x] Implement error handling and structured logging

- [x] Task 6: Create Terraform infrastructure (AC: #4)
  - [x] Define Cloud Function in `terraform/clustering.tf`
  - [x] Create service account with Firestore read/write + GCS write permissions
  - [x] Configure function: Python 3.11, europe-west4, 512MB memory, 5-min timeout
  - [x] Set environment variables: GCP_PROJECT, GCP_REGION, FIRESTORE_COLLECTION
  - [x] Update `terraform/workflows/batch-pipeline.yaml` to add clustering step after Knowledge Cards

- [x] Task 7: Testing and validation (AC: #2, #5, #6, #9)
  - [x] Unit test: clustering algorithm with synthetic embeddings
  - [x] Unit test: Firestore batch write logic (mocked)
  - [x] Integration test: Initial load with 50 test chunks
  - [x] Integration test: Delta processing with 10 new chunks
  - [x] Manual quality check: Review 20 random clusters for semantic coherence
  - [x] Performance test: Measure initial load time (target ≤10 min)
  - [x] Performance test: Measure delta processing time (target ≤5 min)
  - [x] Cost validation: Verify no new AI API calls, only Firestore/GCS costs

- [x] Task 8: Documentation and deployment (AC: #8)
  - [x] Document clustering algorithm choice and parameters in Dev Notes
  - [x] Update architecture.md with clustering step in pipeline
  - [x] Create README for src/clustering module
  - [x] Deploy Cloud Function via `terraform apply`
  - [x] Run initial load script: `python3 -m src.clustering.initial_load`
  - [x] Validate graph.json exported to Cloud Storage

## Dev Notes

### Architecture & Constraints

**Two Execution Modes:**

1. **Initial Load Mode (One-Time Bulk Processing)**
   - **Purpose:** Cluster all existing 818 chunks that don't have cluster assignments yet
   - **Execution:** Local Python script run manually: `python3 -m src.clustering.initial_load`
   - **Data Flow:**
     - Load all chunks with embeddings from Firestore
     - Run clustering algorithm on all 818 embeddings
     - Direct Firestore batch updates (kb_items.cluster_id field)
     - Generate graph.json and upload to Cloud Storage
   - **Idempotency:** Can be re-run to recompute clusters (clears existing cluster_ids first)
   - **Use Case:** Initial setup, or full recluster after algorithm changes

2. **Delta Processing Mode (Daily Pipeline Integration)**
   - **Purpose:** Assign clusters to newly added chunks from daily ingestion
   - **Execution:** Cloud Function automatically triggered by Cloud Workflows after Knowledge Cards step
   - **Data Flow:**
     - Receive batch of new chunk IDs via HTTP POST
     - Load existing cluster centroids/representatives
     - Assign each new chunk to nearest cluster OR create new cluster
     - Batch update Firestore with cluster assignments
     - Update graph.json in Cloud Storage
   - **Efficiency:** Only processes new chunks (typically <50/day), reuses existing cluster structure
   - **Use Case:** Ongoing daily pipeline operation

**Pipeline Integration:**
- Pipeline flow: Ingest → Normalize → Embed → Knowledge Cards → **Clustering** → Export → Digest
- Cloud Function invoked by Cloud Workflows after Knowledge Cards completes
- Workflow passes list of newly processed chunk IDs to clustering function
- Clustering function must complete within 5-minute timeout for daily batches

**Firestore Schema Extension:**
```javascript
// kb_items collection - add cluster_id field:
{
  id: "chunk-abc123",
  content: "Full chunk text...",
  embedding: [...]  // Existing 768-dim vector
  knowledge_card: {...},  // From Story 2.1

  // NEW FIELD:
  cluster_id: ["cluster-42", "cluster-17"]  // Array: chunks can belong to multiple clusters
}
```

**Optional kb_clusters Collection:**
```javascript
// Optional: Store cluster metadata for UI/export
{
  id: "cluster-42",
  label: "AI Safety & Alignment",  // Could be generated later with LLM
  member_count: 23,
  representative_chunk_id: "chunk-xyz",  // Most central chunk
  created_at: Timestamp,
  last_updated: Timestamp
}
```

**Graph Export (graph.json):**
```json
{
  "nodes": [
    {"id": "chunk-abc", "cluster_id": "cluster-42", "title": "..."},
    ...
  ],
  "edges": [
    {"source": "chunk-abc", "target": "chunk-xyz", "weight": 0.85},
    ...
  ],
  "clusters": [
    {"id": "cluster-42", "label": "AI Safety", "size": 23},
    ...
  ]
}
```

### Clustering Algorithm Selection (Web Research January 2025)

**Research Findings:**

After evaluating clustering algorithms for 768-dimensional embeddings with semantic similarity:

| Algorithm | Pros | Cons | Best For |
|-----------|------|------|----------|
| **HDBSCAN** | • No need to predefine cluster count<br>• Finds clusters of varying densities<br>• Automatically identifies noise points<br>• Robust to parameter selection<br>• Now part of scikit-learn (v1.3+) | • Slower than K-Means<br>• May classify too many points as noise if poorly tuned | **Discovering hidden patterns**, variable-density clusters |
| **K-Means** | • Fast and scalable<br>• Simple, well-understood<br>• Works well with MiniBatchKMeans for large datasets | • Requires predefined k (number of clusters)<br>• Assumes spherical clusters<br>• Sensitive to initialization | **Quick results** when approximate cluster count is known |

**Recommended Approach: HDBSCAN**

**Rationale:**
1. **No predefined cluster count:** We don't know how many semantic topics exist in 818 chunks
2. **Variable density:** Some topics may have many related chunks, others just a few
3. **Noise detection:** HDBSCAN can identify outlier chunks that don't fit any cluster
4. **Cosine similarity support:** HDBSCAN supports precomputed distance matrices

**Implementation (Web Research Best Practices):**
```python
from sklearn.metrics.pairwise import cosine_distances
from sklearn.cluster import HDBSCAN  # Now part of scikit-learn 1.3+

# Compute cosine distance matrix from embeddings
distance_matrix = cosine_distances(embeddings)  # 818 x 818 for initial load

# Run HDBSCAN with cosine distance
clusterer = HDBSCAN(
    metric='precomputed',
    min_cluster_size=3,     # Minimum 3 chunks per cluster
    min_samples=2,          # Controls conservativeness
    cluster_selection_epsilon=0.1  # Merge clusters within this distance
)

cluster_labels = clusterer.fit_predict(distance_matrix)
# cluster_labels: array of cluster IDs, -1 = noise
```

**Alternative: K-Means (Fallback)**

If HDBSCAN performance is inadequate or produces too many noise points:

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize

# Normalize embeddings (L2 normalization)
normalized_embeddings = normalize(embeddings, norm='l2')

# Use K-Means with estimated cluster count (e.g., sqrt(818) ≈ 29)
k = 29
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(normalized_embeddings)
```

**Decision:** Start with HDBSCAN for initial implementation. If cluster quality is poor (<80% semantic coherence), evaluate K-Means as fallback.

[Source: Web Research Jan 2025 - scikit-learn HDBSCAN docs, clustering best practices for embeddings]

### Firestore Batch Update Best Practices (Web Research January 2025)

**Key Findings:**

1. **Batch Size Limit:** Maximum 500 operations per batch
2. **Performance:** Batch writes are significantly faster than individual writes (1.71s vs 7.97s in real-world tests)
3. **Error Handling:** Always use try-catch blocks and log errors for debugging

**Implementation Pattern:**
```python
from google.cloud import firestore

db = firestore.Client()
batch = db.batch()
write_count = 0

for i, (chunk_id, cluster_id) in enumerate(chunk_cluster_assignments):
    doc_ref = db.collection('kb_items').document(chunk_id)
    batch.update(doc_ref, {'cluster_id': [cluster_id]})
    write_count += 1

    # Commit every 500 operations
    if write_count == 500:
        try:
            batch.commit()
            print(f"Committed batch {i//500 + 1}")
        except Exception as e:
            print(f"Error committing batch: {e}")
            # Handle retry logic here

        # Create new batch
        batch = db.batch()
        write_count = 0

# Commit remaining operations
if write_count > 0:
    try:
        batch.commit()
        print(f"Committed final batch ({write_count} operations)")
    except Exception as e:
        print(f"Error committing final batch: {e}")
```

[Source: Web Research Jan 2025 - Google Cloud Firestore Python client docs, Stack Overflow best practices]

### Cost Analysis

**Zero New AI Costs:**
- Reuses existing 768-dimensional embeddings from Story 1.6
- No new embedding API calls needed
- Clustering computation is local (Python scikit-learn)

**Estimated Costs:**
- **Firestore writes:** 818 initial updates + ~50/day delta = ~2,318 writes/month
  - Cost: 2,318 × $0.18 per 100k = $0.004/month
- **Firestore reads:** Load embeddings for clustering = ~1,000 reads/month
  - Cost: 1,000 × $0.06 per 100k = $0.0006/month
- **Cloud Storage (graph.json):** ~1MB file, daily updates
  - Cost: Negligible (<$0.001/month)
- **Cloud Function execution:** ~30 invocations/month × 2 minutes avg
  - Cost: 30 × 2 × $0.0000025 per vCPU-second = $0.0001/month
- **Total:** ~$0.005/month

✅ **Well under AC #9 requirement (≤$0.10/month)**

### Learnings from Previous Story

**From Story 2.1-knowledge-cards (Status: done)**

**Files Created (Reuse Patterns):**
- `src/knowledge_cards/generator.py` - Batch processing with progress logging, retry logic
  - **Reuse:** Follow same batch processing pattern for clustering script
- `src/knowledge_cards/main.py` - CLI entry point with argument parsing
  - **Reuse:** Similar CLI structure for initial load script
- `terraform/knowledge_cards.tf` - Cloud Function infrastructure
  - **Reuse:** Template for clustering Cloud Function Terraform config
- `tests/test_integration_knowledge_cards.py` - Integration test patterns
  - **Reuse:** Similar integration test structure for clustering

**Architectural Patterns Established:**
- **Two execution modes:** Local script (initial/one-time) + Cloud Function (pipeline integration)
  - **Apply:** Same pattern for clustering - initial load script + delta processing function
- **Firestore batch writes:** Used 100 updates/batch for knowledge cards
  - **Apply:** Use 500 updates/batch for clustering (Firestore max batch size)
- **Progress logging:** Log every 10 chunks processed
  - **Apply:** Log every 100 chunks for clustering (larger batch)
- **Error handling:** Exponential backoff retry logic for API failures
  - **Apply:** Similar retry logic for Firestore batch write failures

**Cloud Function Deployment:**
- Successfully deployed to europe-west4 with Terraform
- IAM permissions: Service account needs Firestore read/write + GCS write
- Environment variables pattern: `GCP_PROJECT`, `GCP_REGION`, `FIRESTORE_COLLECTION`
- Workflow integration: HTTP POST trigger with JSON payload

**Technical Debt / Warnings:**
- Knowledge cards function uses `gemini-2.5-flash` (not `gemini-2.5-flash-lite` as originally planned)
  - **Note:** Web research shows latest model naming conventions may differ from initial docs
  - **Action:** Always verify model availability in europe-west4 before implementation
- Initial generation took ~70 minutes due to API rate limits
  - **Note:** Clustering should be much faster (no AI API calls, pure computation)

**Testing Approach:**
- Comprehensive unit tests (22 tests) with mocked external dependencies
- Integration tests with small sample datasets (10-50 items)
- Manual quality validation with 20 random samples
- Cost tracking built into test runs

**Performance Considerations:**
- Batch processing with progress tracking essential for user visibility
- Exponential backoff crucial for handling transient Firestore errors
- Idempotency check: Ensure reruns don't duplicate data

[Source: stories/2.1-knowledge-cards.md#Dev-Agent-Record]

### Project Structure Notes

**New Files (Expected):**
- `src/clustering/__init__.py` - Package init
- `src/clustering/clusterer.py` - Core clustering logic (HDBSCAN/K-Means)
- `src/clustering/initial_load.py` - CLI script for bulk clustering all chunks
- `src/clustering/cloud_function.py` - Cloud Function HTTP handler for delta processing
- `src/clustering/graph_generator.py` - Generate graph.json from cluster assignments
- `terraform/clustering.tf` - Infrastructure as code for Cloud Function
- `tests/test_clusterer.py` - Unit tests for clustering algorithms
- `tests/test_integration_clustering.py` - Integration tests (initial load + delta)

**Modified Files:**
- `terraform/workflows/batch-pipeline.yaml` - Add clustering step after knowledge cards
- `docs/architecture.md` - Update pipeline diagram with clustering step

**Dependencies:**
- `scikit-learn>=1.3.0` - Includes HDBSCAN (AC #2)
- `google-cloud-firestore>=2.16.0` - Already installed (Story 1.3)
- `google-cloud-storage>=2.10.0` - Already installed (Story 1.1)
- `numpy>=1.24.0` - For array operations (already installed)

**Alignment with Existing Structure:**
- Follows pattern from `src/knowledge_cards/` module structure
- Reuses Firestore client patterns from Epic 1 stories
- Similar Cloud Function deployment pattern (Terraform + service account)

### Testing Strategy

**Unit Tests:**
- Test HDBSCAN clustering with synthetic embeddings (10 samples)
- Test K-Means clustering fallback with synthetic data
- Test Firestore batch write logic with mocked client
- Test graph.json generation with sample cluster assignments
- Test cosine distance matrix computation

**Integration Tests:**
- Run initial load on 50 test chunks from staging Firestore
- Verify all 50 chunks assigned cluster_ids
- Run delta processing with 10 new chunks
- Verify new chunks assigned to existing clusters
- Validate graph.json structure and content

**Quality Validation:**
- Manually review 20 random clusters
- For each cluster: read 3-5 member chunks, assess semantic coherence
- Calculate % of clusters with ≥80% coherent members
- Target: ≥80% of clusters pass coherence check (AC #2)
- If <80%, iterate on algorithm parameters or switch to K-Means

**Performance Testing:**
- Measure initial load time for 818 chunks
- Target: ≤10 minutes (AC #5)
- Measure delta processing time for batch of 50 new chunks
- Target: ≤5 minutes (AC #6)
- Profile: Identify bottlenecks (Firestore reads, distance computation, Firestore writes)

**Cost Validation:**
- Monitor Firestore read/write operations during test runs
- Verify zero new embedding API calls
- Calculate projected monthly cost
- Target: ≤$0.10/month (AC #9)

### References

- [Source: docs/epics.md#Story-2.2] - Story requirements and technical approach
- [Source: docs/PRD.md#Data-Flows] - Pipeline integration (step 5: Cluster & Link)
- [Source: docs/architecture.md#Batch-Pipeline] - Cloud Workflows orchestration
- [Source: Web Research Jan 2025 - HDBSCAN docs] - Clustering algorithm best practices for embeddings
- [Source: Web Research Jan 2025 - scikit-learn docs] - K-Means clustering with high-dimensional vectors
- [Source: Web Research Jan 2025 - Google Cloud docs] - Firestore batch write performance and limits
- [Source: stories/2.1-knowledge-cards.md] - Patterns for two-mode execution (local + Cloud Function)

## Dev Agent Record

### Context Reference

- docs/stories/2-2-semantic-clustering.context.xml

### Agent Model Used

{{agent_model_name_version}}

### Debug Log References

### Completion Notes List

### File List
