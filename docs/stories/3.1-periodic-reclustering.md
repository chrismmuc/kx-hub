# Story 3.1: Automated Periodic Re-clustering with Drift Detection

## Status: PLANNED
**Epic:** 3 - Advanced Knowledge Graph & System Optimization
**Dependencies:** Story 2.3 (Clustering Consistency Fix)
**Priority:** MEDIUM (Quality optimization, not critical)

---

## Problem Statement

### The Cluster Drift Problem

After implementing UMAP-consistent delta clustering (Story 2.3), we face a new challenge: **cluster boundaries and centroids become stale over time** as the knowledge base grows.

#### Current Limitations (Post-Story 2.3)

| Aspect | Current Behavior | Impact |
|--------|------------------|--------|
| **UMAP Model** | Frozen at initial corpus size (823 chunks) | New semantic spaces not well-represented in 5D projection |
| **Centroids** | Fixed from initial clustering | Don't adapt to new chunks in cluster |
| **Cluster Boundaries** | Never updated | Can't split over-broad clusters or merge similar ones |
| **New Topics** | Forced into existing clusters | Semantically unrelated chunks grouped together |

#### Concrete Example of Drift

**Initial State (Nov 2024):**
```
Cluster 12: "Productivity Systems" (45 chunks)
  - Topics: GTD, Time blocking, Task management
  - Centroid (5D): [1.2, -0.5, 0.3, 2.1, -1.0]
  - Coherence: High (silhouette score: 0.72)
```

**After 6 Months (May 2025):**
```
Cluster 12: "Productivity Systems" (103 chunks)
  - Original topics: GTD, Time blocking, Task management
  - New topics added via delta:
    - Personal knowledge management (Zettelkasten, Obsidian)
    - AI-assisted productivity tools
    - Focus and deep work techniques
  - Centroid (5D): [1.2, -0.5, 0.3, 2.1, -1.0]  ❌ UNCHANGED
  - Coherence: Low (silhouette score: 0.43)  ❌ DEGRADED

Problem: Should be 2-3 distinct clusters, but remains as one!
```

---

## Solution Architecture

### Core Principle: Automated Cluster Evolution

Implement **automated periodic re-clustering** that:
1. Detects when cluster quality has degraded
2. Triggers full re-clustering on updated corpus
3. Maps old cluster IDs to new cluster IDs for continuity
4. Updates UMAP model and centroids
5. Preserves cluster names/metadata where possible

---

## System Design

### High-Level Architecture

```
┌────────────────────────────────────────────────────────────────┐
│                  PERIODIC RE-CLUSTERING SYSTEM                  │
├────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────┐     │
│  │  1. DRIFT DETECTION (Weekly Cloud Scheduler)         │     │
│  │     - Monitor cluster quality metrics                │     │
│  │     - Track corpus growth rate                       │     │
│  │     - Check days since last re-clustering            │     │
│  │     - Manual override flag                           │     │
│  └──────────────────────────────────────────────────────┘     │
│                           │                                     │
│                           │ Trigger if needed                   │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────┐     │
│  │  2. RE-CLUSTERING JOB (Cloud Run)                    │     │
│  │     - Load full corpus (all chunks)                  │     │
│  │     - Re-train UMAP on full dataset                  │     │
│  │     - Re-run HDBSCAN clustering                      │     │
│  │     - Map old cluster IDs → new cluster IDs          │     │
│  │     - Update all kb_items in Firestore               │     │
│  │     - Regenerate cluster metadata (if needed)        │     │
│  │     - Save new UMAP model to Cloud Storage           │     │
│  └──────────────────────────────────────────────────────┘     │
│                           │                                     │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────┐     │
│  │  3. DELTA CLUSTERING (Resumes with new model)        │     │
│  │     - Loads updated UMAP model                       │     │
│  │     - Uses updated 5D centroids                      │     │
│  │     - Cluster quality maintained                     │     │
│  └──────────────────────────────────────────────────────┘     │
│                                                                 │
└────────────────────────────────────────────────────────────────┘
```

---

## Technical Implementation

### 1. Drift Detection Triggers

#### Firestore Metadata Schema

```javascript
// Collection: clustering_metadata
// Document: current_state
{
  "last_reclustering_date": "2025-11-02T10:30:00Z",
  "last_reclustering_size": 823,              // Corpus size at last re-clustering
  "last_reclustering_cluster_count": 38,
  "last_umap_version": "umap_v1_20251102",
  "current_corpus_size": 1247,                // Updated daily
  "avg_silhouette_score": 0.68,               // Cluster quality metric
  "force_reclustering": false,                // Manual override
  "reclustering_in_progress": false,
  "updated_at": "2025-11-02T15:45:00Z"
}
```

#### Trigger Logic

```python
def should_trigger_reclustering(db: firestore.Client) -> tuple[bool, str]:
    """
    Determine if re-clustering should be triggered.

    Triggers:
    1. Manual override flag set to True
    2. 90+ days since last re-clustering
    3. 20%+ corpus growth since last re-clustering
    4. Average silhouette score drops below 0.55

    Returns:
        (should_trigger, reason)
    """
    metadata = db.collection('clustering_metadata').document('current_state').get()
    data = metadata.to_dict()

    # Manual override
    if data.get('force_reclustering', False):
        return True, "Manual override flag set"

    # Time-based trigger (quarterly)
    last_date = datetime.fromisoformat(data['last_reclustering_date'].replace('Z', '+00:00'))
    days_since = (datetime.now(timezone.utc) - last_date).days

    if days_since >= 90:
        return True, f"90+ days since last re-clustering ({days_since} days)"

    # Growth-based trigger (20% corpus growth)
    last_size = data['last_reclustering_size']
    current_size = data['current_corpus_size']
    growth_rate = (current_size - last_size) / last_size

    if growth_rate >= 0.20:
        return True, f"20%+ corpus growth ({growth_rate:.1%})"

    # Quality-based trigger (silhouette score degradation)
    avg_score = data.get('avg_silhouette_score', 1.0)

    if avg_score < 0.55:
        return True, f"Cluster quality degraded (silhouette={avg_score:.2f})"

    return False, "No trigger conditions met"
```

#### Cloud Scheduler (Weekly Check)

```python
# functions/clustering/drift_detector.py

@functions_framework.http
def check_reclustering_trigger(request):
    """
    Weekly Cloud Function to check if re-clustering is needed.

    Triggered by: Cloud Scheduler (every Sunday at 2am UTC)
    """
    logger.info("Checking re-clustering triggers...")

    project_id = os.getenv('GCP_PROJECT')
    db = firestore.Client(project=project_id)

    # Check if re-clustering already in progress
    metadata = db.collection('clustering_metadata').document('current_state').get()
    if metadata.get('reclustering_in_progress', False):
        logger.info("Re-clustering already in progress, skipping")
        return {'status': 'in_progress'}, 200

    # Check triggers
    should_trigger, reason = should_trigger_reclustering(db)

    if should_trigger:
        logger.info(f"Triggering re-clustering: {reason}")

        # Set in-progress flag
        db.collection('clustering_metadata').document('current_state').update({
            'reclustering_in_progress': True,
            'reclustering_trigger_reason': reason,
            'reclustering_started_at': datetime.utcnow().isoformat() + 'Z'
        })

        # Trigger Cloud Run job
        trigger_cloud_run_job('reclustering-job', {'reason': reason})

        return {
            'status': 'triggered',
            'reason': reason
        }, 200
    else:
        logger.info(f"Re-clustering not needed: {reason}")
        return {
            'status': 'not_needed',
            'reason': reason
        }, 200
```

---

### 2. Re-clustering Cloud Run Job

#### Why Cloud Run (not Cloud Function)

**Cloud Functions limitations:**
- Max execution time: 9 minutes (3rd gen) / 60 minutes (2nd gen)
- Limited memory: 8GB max
- Cold starts

**Cloud Run advantages:**
- Max execution time: 60 minutes
- Memory: Up to 32GB
- More suitable for batch processing
- Can run complex ML operations (UMAP re-training)

#### Job Implementation

```python
# cloud_run/reclustering_job/main.py

import logging
import time
from typing import Dict, Any, List
import numpy as np
from google.cloud import firestore
from google.cloud import storage
import pickle

# Import from shared module
from clustering.clusterer import SemanticClusterer
from clustering.cluster_metadata import ClusterMetadataGenerator

logger = logging.getLogger(__name__)


class ReclusteringJob:
    """
    Handles full corpus re-clustering with cluster ID mapping.
    """

    def __init__(self, project_id: str, reason: str):
        self.project_id = project_id
        self.reason = reason
        self.db = firestore.Client(project=project_id)
        self.storage_client = storage.Client(project=project_id)

    def run(self) -> Dict[str, Any]:
        """
        Execute full re-clustering workflow.

        Returns:
            Summary statistics
        """
        logger.info("=" * 70)
        logger.info("AUTOMATED RE-CLUSTERING JOB - START")
        logger.info(f"Trigger reason: {self.reason}")
        logger.info("=" * 70)

        start_time = time.time()

        try:
            # Step 1: Load old cluster assignments (for mapping)
            logger.info("\n[Step 1/8] Loading old cluster assignments...")
            old_assignments = self._load_old_assignments()

            # Step 2: Load full corpus
            logger.info("\n[Step 2/8] Loading full corpus...")
            chunks, embeddings, chunk_ids = self._load_full_corpus()
            logger.info(f"Loaded {len(chunks)} chunks with embeddings")

            # Step 3: Re-train UMAP on full corpus
            logger.info("\n[Step 3/8] Re-training UMAP on full corpus...")
            clusterer = SemanticClusterer(
                algorithm='hdbscan',
                min_cluster_size=10,
                min_samples=3,
                use_umap=True,
                umap_n_components=5
            )
            new_labels = clusterer.fit_predict(embeddings)
            logger.info(f"Clustering complete: {clusterer.n_clusters_found} clusters found")

            # Step 4: Map old cluster IDs to new cluster IDs
            logger.info("\n[Step 4/8] Mapping old clusters to new clusters...")
            cluster_mapping = self._map_old_to_new_clusters(
                old_assignments, new_labels, chunk_ids
            )

            # Step 5: Update Firestore with new assignments
            logger.info("\n[Step 5/8] Updating Firestore with new cluster assignments...")
            self._update_firestore_assignments(chunk_ids, new_labels, cluster_mapping)

            # Step 6: Save new UMAP model
            logger.info("\n[Step 6/8] Saving new UMAP model to Cloud Storage...")
            umap_version = self._save_umap_model(clusterer.umap_model)

            # Step 7: Regenerate cluster metadata
            logger.info("\n[Step 7/8] Regenerating cluster metadata...")
            self._regenerate_cluster_metadata(cluster_mapping)

            # Step 8: Update clustering metadata
            logger.info("\n[Step 8/8] Updating clustering metadata...")
            elapsed = time.time() - start_time
            self._update_metadata(len(chunks), clusterer.n_clusters_found, umap_version)

            logger.info("\n" + "=" * 70)
            logger.info("AUTOMATED RE-CLUSTERING JOB - COMPLETE")
            logger.info(f"Duration: {elapsed:.1f} seconds")
            logger.info("=" * 70)

            return {
                'status': 'success',
                'chunks_reclustered': len(chunks),
                'old_cluster_count': len(set(old_assignments.values())),
                'new_cluster_count': clusterer.n_clusters_found,
                'clusters_preserved': sum(1 for v in cluster_mapping.values() if v != 'new'),
                'duration_seconds': round(elapsed, 1),
                'umap_version': umap_version
            }

        except Exception as e:
            logger.error(f"Re-clustering failed: {e}", exc_info=True)

            # Clear in-progress flag
            self.db.collection('clustering_metadata').document('current_state').update({
                'reclustering_in_progress': False,
                'last_reclustering_error': str(e),
                'last_reclustering_error_at': datetime.utcnow().isoformat() + 'Z'
            })

            raise

    def _load_old_assignments(self) -> Dict[str, str]:
        """
        Load current cluster assignments before re-clustering.

        Returns:
            Dict mapping chunk_id → cluster_id
        """
        old_assignments = {}

        docs = self.db.collection('kb_items').stream()
        for doc in docs:
            data = doc.to_dict()
            cluster_ids = data.get('cluster_id', [])
            if cluster_ids:
                old_assignments[doc.id] = cluster_ids[0]  # Primary cluster

        return old_assignments

    def _load_full_corpus(self) -> tuple[List[Dict], np.ndarray, List[str]]:
        """
        Load all chunks with embeddings from Firestore.

        Returns:
            (chunks, embeddings_array, chunk_ids)
        """
        chunks = []
        embeddings = []
        chunk_ids = []

        docs = self.db.collection('kb_items').stream()

        for doc in docs:
            data = doc.to_dict()

            # Check for embedding
            if 'embedding' not in data:
                continue

            # Extract embedding
            embedding = data['embedding']
            if hasattr(embedding, 'to_map_value'):
                map_value = embedding.to_map_value()
                embedding_array = np.array(map_value.get('value', map_value), dtype=np.float32)
            elif isinstance(embedding, list):
                embedding_array = np.array(embedding, dtype=np.float32)
            else:
                continue

            if len(embedding_array) != 768:
                continue

            chunks.append(data)
            embeddings.append(embedding_array)
            chunk_ids.append(doc.id)

        return chunks, np.array(embeddings), chunk_ids

    def _map_old_to_new_clusters(
        self,
        old_assignments: Dict[str, str],
        new_labels: np.ndarray,
        chunk_ids: List[str]
    ) -> Dict[int, str]:
        """
        Map new cluster labels to old cluster IDs based on overlap.

        Strategy:
        - If new cluster has >70% overlap with old cluster, preserve old ID
        - Otherwise, create new cluster ID

        Returns:
            Dict mapping new_label → old_cluster_id or 'new'
        """
        mapping = {}

        unique_new_labels = np.unique(new_labels)

        for new_label in unique_new_labels:
            if new_label == -1:  # Noise cluster
                mapping[new_label] = 'noise'
                continue

            # Get chunks in this new cluster
            indices = np.where(new_labels == new_label)[0]
            chunks_in_new = [chunk_ids[i] for i in indices]

            # Count overlap with each old cluster
            old_cluster_overlap = {}
            for chunk_id in chunks_in_new:
                old_cluster = old_assignments.get(chunk_id)
                if old_cluster:
                    old_cluster_overlap[old_cluster] = old_cluster_overlap.get(old_cluster, 0) + 1

            if not old_cluster_overlap:
                # Completely new cluster
                mapping[new_label] = 'new'
                continue

            # Find old cluster with highest overlap
            best_old_cluster = max(old_cluster_overlap, key=old_cluster_overlap.get)
            overlap_count = old_cluster_overlap[best_old_cluster]
            overlap_ratio = overlap_count / len(chunks_in_new)

            if overlap_ratio >= 0.70:
                # Preserve old cluster ID
                mapping[new_label] = best_old_cluster
                logger.info(f"  New cluster {new_label} → {best_old_cluster} ({overlap_ratio:.1%} overlap)")
            else:
                # Create new cluster ID
                mapping[new_label] = 'new'
                logger.info(f"  New cluster {new_label} → NEW cluster ({overlap_ratio:.1%} best overlap)")

        return mapping

    def _update_firestore_assignments(
        self,
        chunk_ids: List[str],
        new_labels: np.ndarray,
        cluster_mapping: Dict[int, str]
    ):
        """
        Update all kb_items with new cluster assignments.
        """
        # Generate new cluster IDs for 'new' clusters
        existing_cluster_ids = set(cluster_mapping.values()) - {'new', 'noise'}
        next_cluster_num = max([int(cid.split('-')[1]) for cid in existing_cluster_ids if cid.startswith('cluster-')], default=0) + 1

        new_cluster_assignments = {}

        batch = self.db.batch()
        write_count = 0

        for i, chunk_id in enumerate(chunk_ids):
            new_label = new_labels[i]
            mapped_cluster = cluster_mapping[new_label]

            # Assign cluster ID
            if mapped_cluster == 'noise':
                cluster_id = 'noise'
            elif mapped_cluster == 'new':
                # Generate new cluster ID
                if new_label not in new_cluster_assignments:
                    new_cluster_assignments[new_label] = f'cluster-{next_cluster_num}'
                    next_cluster_num += 1
                cluster_id = new_cluster_assignments[new_label]
            else:
                cluster_id = mapped_cluster

            # Update document
            doc_ref = self.db.collection('kb_items').document(chunk_id)
            batch.update(doc_ref, {'cluster_id': [cluster_id]})
            write_count += 1

            # Commit every 500 operations
            if write_count == 500:
                batch.commit()
                batch = self.db.batch()
                write_count = 0

        # Commit remaining
        if write_count > 0:
            batch.commit()

        logger.info(f"Updated {len(chunk_ids)} chunks with new cluster assignments")

    def _save_umap_model(self, umap_model) -> str:
        """
        Save UMAP model to Cloud Storage with version.

        Returns:
            Version string (e.g., 'umap_v2_20251102')
        """
        from datetime import datetime

        version = f"umap_v2_{datetime.utcnow().strftime('%Y%m%d')}"

        # Pickle UMAP model
        model_bytes = pickle.dumps(umap_model)

        # Upload to Cloud Storage
        bucket = self.storage_client.bucket(f'{self.project_id}-pipeline')
        blob = bucket.blob(f'models/{version}.pkl')
        blob.upload_from_string(model_bytes)

        logger.info(f"Saved UMAP model: gs://{bucket.name}/models/{version}.pkl")

        return version

    def _regenerate_cluster_metadata(self, cluster_mapping: Dict[int, str]):
        """
        Regenerate cluster metadata (names, descriptions, centroids).

        For preserved clusters: Keep existing names
        For new clusters: Generate new names via Gemini
        """
        # Use ClusterMetadataGenerator
        metadata_gen = ClusterMetadataGenerator(
            project_id=self.project_id,
            region='europe-west4',
            kb_collection='kb_items',
            clusters_collection='clusters'
        )

        # Generate metadata for all clusters
        metadata_gen.generate_all_clusters()

    def _update_metadata(self, corpus_size: int, cluster_count: int, umap_version: str):
        """
        Update clustering_metadata with new state.
        """
        self.db.collection('clustering_metadata').document('current_state').update({
            'last_reclustering_date': datetime.utcnow().isoformat() + 'Z',
            'last_reclustering_size': corpus_size,
            'last_reclustering_cluster_count': cluster_count,
            'last_umap_version': umap_version,
            'current_corpus_size': corpus_size,
            'reclustering_in_progress': False,
            'force_reclustering': False,
            'updated_at': datetime.utcnow().isoformat() + 'Z'
        })


# Flask app for Cloud Run
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/run', methods=['POST'])
def run_reclustering():
    """
    Cloud Run endpoint for re-clustering job.
    """
    request_json = request.get_json(silent=True)
    reason = request_json.get('reason', 'Manual trigger') if request_json else 'Manual trigger'

    project_id = os.getenv('GCP_PROJECT')

    job = ReclusteringJob(project_id, reason)
    result = job.run()

    return jsonify(result), 200


if __name__ == '__main__':
    port = int(os.getenv('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
```

---

### 3. Infrastructure Setup

#### Cloud Scheduler Configuration

```yaml
# terraform/cloud_scheduler.tf

resource "google_cloud_scheduler_job" "reclustering_check" {
  name             = "reclustering-drift-check"
  description      = "Weekly check for cluster drift triggers"
  schedule         = "0 2 * * 0"  # Every Sunday at 2am UTC
  time_zone        = "UTC"
  attempt_deadline = "320s"

  http_target {
    http_method = "POST"
    uri         = "https://europe-west4-${var.project_id}.cloudfunctions.net/drift-detector"

    oidc_token {
      service_account_email = google_service_account.pipeline.email
    }
  }
}
```

#### Cloud Run Job Configuration

```yaml
# terraform/cloud_run.tf

resource "google_cloud_run_v2_job" "reclustering" {
  name     = "reclustering-job"
  location = var.region

  template {
    template {
      containers {
        image = "gcr.io/${var.project_id}/reclustering-job:latest"

        resources {
          limits = {
            cpu    = "4"
            memory = "16Gi"
          }
        }

        env {
          name  = "GCP_PROJECT"
          value = var.project_id
        }
      }

      timeout         = "3600s"  # 60 minutes max
      max_retries     = 1
      service_account = google_service_account.pipeline.email
    }
  }
}
```

---

## Performance & Cost Analysis

### Expected Performance

| Corpus Size | UMAP Training | HDBSCAN | Firestore Updates | Total Time |
|-------------|---------------|---------|-------------------|------------|
| 1,000 chunks | ~5 seconds | ~3 seconds | ~2 seconds | **~10 seconds** |
| 5,000 chunks | ~25 seconds | ~15 seconds | ~10 seconds | **~50 seconds** |
| 10,000 chunks | ~60 seconds | ~40 seconds | ~20 seconds | **~2 minutes** |
| 50,000 chunks | ~5 minutes | ~4 minutes | ~2 minutes | **~11 minutes** |

### Cost Analysis

#### Per Re-clustering Cost

| Component | Cost | Frequency | Monthly Cost |
|-----------|------|-----------|--------------|
| Cloud Run (CPU) | $0.10 per re-cluster | Quarterly | **$0.03/month** |
| Firestore reads | $0.01 | Quarterly | **$0.003/month** |
| Firestore writes | $0.02 | Quarterly | **$0.007/month** |
| Cloud Storage (UMAP model) | $0.001 | One-time | **$0.001/month** |
| Gemini API (cluster names) | $0.10 | Only for new clusters | **~$0.03/month** |
| **Total** | | | **~$0.07/month** |

#### Annual Cost: **~$0.84/year**

**Note:** Cost scales with corpus size, but remains <$5/year even at 50,000 chunks.

---

## Acceptance Criteria

- [ ] Drift detection Cloud Function deployed
- [ ] Weekly Cloud Scheduler job configured
- [ ] Re-clustering Cloud Run job deployed and tested
- [ ] Cluster ID mapping preserves >90% of old cluster IDs (70%+ overlap)
- [ ] UMAP model versioning implemented
- [ ] Firestore metadata schema created
- [ ] Manual trigger mechanism works (`force_reclustering` flag)
- [ ] Re-clustering completes in <5 minutes for 5,000 chunks
- [ ] Delta clustering automatically picks up new UMAP model
- [ ] Cluster quality metrics logged after each re-clustering
- [ ] Documentation updated with re-clustering workflow

---

## Testing Strategy

### Unit Tests

```python
def test_drift_detection_triggers():
    """Test that triggers fire correctly."""
    # Time-based trigger
    assert should_trigger_reclustering(mock_db(days_since=95)) == (True, "90+ days...")

    # Growth-based trigger
    assert should_trigger_reclustering(mock_db(growth=0.25)) == (True, "20%+ corpus...")

    # Quality-based trigger
    assert should_trigger_reclustering(mock_db(silhouette=0.52)) == (True, "Cluster quality...")

    # No trigger
    assert should_trigger_reclustering(mock_db(days_since=30, growth=0.05)) == (False, "No trigger...")

def test_cluster_mapping_preserves_ids():
    """Test cluster ID mapping logic."""
    old_assignments = {'chunk1': 'cluster-5', 'chunk2': 'cluster-5', 'chunk3': 'cluster-8'}
    new_labels = np.array([0, 0, 1])
    chunk_ids = ['chunk1', 'chunk2', 'chunk3']

    mapping = map_old_to_new_clusters(old_assignments, new_labels, chunk_ids)

    # New cluster 0 should map to old cluster-5 (100% overlap)
    assert mapping[0] == 'cluster-5'
```

### Integration Tests

```python
def test_full_reclustering_workflow():
    """End-to-end test of re-clustering."""
    # 1. Setup: Create test corpus with known clusters
    # 2. Run initial clustering
    # 3. Add new chunks
    # 4. Trigger re-clustering
    # 5. Verify cluster IDs mostly preserved
    # 6. Verify new UMAP model saved
    # 7. Verify delta clustering uses new model
```

---

## Migration Plan

### Phase 1: Deploy Infrastructure (Week 1)

1. Deploy drift detector Cloud Function
2. Configure Cloud Scheduler
3. Deploy re-clustering Cloud Run job
4. Create Firestore metadata schema

### Phase 2: Initial Baseline (Week 1)

1. Run re-clustering manually once to establish baseline
2. Verify cluster ID mapping works
3. Set initial metadata values

### Phase 3: Monitoring (Week 2-4)

1. Monitor drift metrics weekly
2. Tune trigger thresholds based on observed drift
3. Adjust re-clustering frequency if needed

---

## Monitoring & Alerts

### Cloud Monitoring Metrics

```python
# Log metrics after each re-clustering
from google.cloud import monitoring_v3

def log_reclustering_metrics(result: Dict):
    """
    Log custom metrics to Cloud Monitoring.
    """
    client = monitoring_v3.MetricServiceClient()
    project_name = f"projects/{project_id}"

    # Cluster count metric
    series = monitoring_v3.TimeSeries()
    series.metric.type = 'custom.googleapis.com/clustering/cluster_count'
    point = monitoring_v3.Point()
    point.value.int64_value = result['new_cluster_count']
    point.interval.end_time.seconds = int(time.time())
    series.points = [point]

    client.create_time_series(name=project_name, time_series=[series])
```

### Recommended Alerts

1. **Re-clustering failure:** Alert if re-clustering job fails
2. **Long re-clustering duration:** Alert if re-clustering takes >10 minutes
3. **Low cluster preservation:** Alert if <70% of clusters preserved
4. **Drift detection disabled:** Alert if weekly check hasn't run in 10 days

---

## Risks and Mitigations

### Risk 1: Re-clustering During Active Usage

**Risk:** Re-clustering updates cluster IDs while users are browsing/searching

**Mitigation:**
- Run re-clustering during low-usage window (Sundays 2am UTC)
- Use Firestore transactions for atomic updates
- MCP server caches cluster metadata (eventual consistency acceptable)

### Risk 2: Cluster ID Churn

**Risk:** Even with 70% overlap threshold, some clusters change IDs frequently

**Mitigation:**
- Track cluster ID changes over time
- If cluster changes ID >3 times in 6 months, manually stabilize
- Consider increasing overlap threshold to 80%

### Risk 3: UMAP Non-determinism

**Risk:** UMAP with same random_state can produce slightly different results on different runs

**Mitigation:**
- Accept minor variance as acceptable (clusters will be semantically similar)
- Use cluster ID mapping to preserve continuity
- Monitor cluster quality metrics (silhouette score) to detect degradation

---

## Future Enhancements

### 1. Incremental Centroid Updates (Between Re-clusterings)

Add lightweight centroid updates during delta clustering:

```python
# After assigning new chunk to cluster
def update_centroid_incremental(cluster_id, new_embedding_5d, current_size):
    cluster_doc = db.collection('clusters').document(cluster_id)
    current_centroid = cluster_doc.get().get('centroid_5d')

    updated_centroid = (current_centroid * current_size + new_embedding_5d) / (current_size + 1)

    cluster_doc.update({
        'centroid_5d': updated_centroid,
        'size': current_size + 1
    })
```

**Benefit:** Reduces drift between re-clusterings

### 2. A/B Testing for Cluster Quality

Compare cluster quality before/after re-clustering:

```python
def evaluate_cluster_quality_improvement(old_labels, new_labels, embeddings):
    """
    Measure if re-clustering improved cluster quality.
    """
    old_score = silhouette_score(embeddings, old_labels)
    new_score = silhouette_score(embeddings, new_labels)

    improvement = new_score - old_score

    logger.info(f"Cluster quality: {old_score:.3f} → {new_score:.3f} (Δ{improvement:+.3f})")

    return improvement
```

### 3. Smart Trigger Tuning

Use ML to predict optimal re-clustering frequency:

```python
# Track drift rate over time
# Predict when silhouette score will drop below 0.55
# Trigger re-clustering proactively before quality degrades
```

---

## References

- [UMAP Documentation: Transforming New Data](https://umap-learn.readthedocs.io/en/latest/transform.html)
- [HDBSCAN: Hierarchical Density-Based Clustering](https://hdbscan.readthedocs.io/)
- [Cloud Run Jobs](https://cloud.google.com/run/docs/create-jobs)
- [Silhouette Score for Cluster Quality](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)

---

**Story 3.1 ready for review and implementation**
**Estimated effort:** 2-3 weeks (includes testing and monitoring setup)
**Business value:** Maintains cluster quality long-term, enables KB growth to 50,000+ chunks
