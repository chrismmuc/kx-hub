# Story 3.1: Automated Periodic Re-clustering with Drift Detection

## Status: PLANNED (Defer Until Scale Justifies)
**Epic:** 3 - Advanced Knowledge Graph & System Optimization
**Dependencies:** Story 2.3 (Clustering Consistency Fix) âœ… COMPLETE
**Priority:** LOW (Quality optimization, manual process sufficient for 15-30 months)

**ðŸ“š Reference:** See [Clustering Reclustering Strategy](../clustering-reclustering-strategy.md) for detailed strategic analysis

---

## TL;DR - Executive Summary

**Problem:** UMAP's "frozen manifold" becomes outdated as the knowledge base grows, requiring periodic full reclustering to maintain quality.

**Current Solution (Phase 0):** Manual reclustering via `python3 -m src.clustering.initial_load`
- âœ… Works well for current 835 chunks
- âœ… Quarterly run takes ~5 minutes
- âœ… Cost: ~$0.04/year

**This Story (Phase 1):** Automated drift detection + scheduled reclustering
- â° Implement when: Corpus hits 5,000+ chunks OR growth >200/month OR manual process breaks
- ðŸ“… Timeline: Not needed for 15-30 months (current growth: ~50-100/month)
- ðŸ’° Cost: ~$0.07/month (when implemented)
- âš™ï¸ Effort: 2-3 weeks

**Recommendation:** **DEFER this story.** Manual reclustering is sufficient for current scale. Revisit when corpus reaches 2,500 chunks or in 12 months.

### Immediate Actions (Phase 0 Maintenance)

**For the next 15-30 months, follow this simple process:**

1. **Set Calendar Reminder:** Quarterly reclustering (every 3 months)
   - Suggested dates: Jan 15, Apr 15, Jul 15, Oct 15

2. **When reminder fires, run:**
   ```bash
   # Set environment
   export GCP_PROJECT=kx-hub
   export GCP_REGION=europe-west4
   export FIRESTORE_COLLECTION=kb_items
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json

   # Run reclustering (~10 seconds)
   python3 -m src.clustering.initial_load

   # Verify success (check logs for "COMPLETE")
   # Expected: New UMAP model saved, all chunks reassigned
   ```

3. **Track metrics:**
   - Note current corpus size (check logs)
   - Note cluster count (check logs)
   - When corpus hits 2,500 chunks â†’ revisit this story

4. **Decision point:**
   - If manual process feels burdensome â†’ implement Phase 1 (this story)
   - If everything runs smoothly â†’ continue Phase 0

**Current approach requires:** 5 minutes per quarter = 20 minutes per year

---

## Problem Statement

### The "Frozen Manifold" Problem

After implementing UMAP-consistent delta clustering (Story 2.3), we face an inherent limitation: **the UMAP model is a "frozen snapshot" of the data distribution at the time of initial clustering**, and this snapshot becomes less representative as the knowledge base grows and evolves.

#### How UMAP Delta Processing Works

```
Initial Clustering (835 chunks):
  â””â”€ UMAP.fit_transform() - Learns manifold structure of 835 chunks
     â””â”€ Creates 2.7 MB model capturing the topology
  â””â”€ Saves "frozen manifold" to Cloud Storage

Delta Processing (new chunks arrive):
  â””â”€ UMAP.transform() - Projects new chunks onto the frozen manifold
     â””â”€ Does NOT update the model
     â””â”€ Assumes new chunks fit the old topology
```

**The Core Issue:** `UMAP.transform()` uses a frozen transformation optimized for the original 835 chunks. As data distribution evolves (new topics, more examples), the frozen manifold becomes outdated, leading to suboptimal 5D projections for new chunks.

#### Why Reclustering is Eventually Required

**Key Insight from Analysis:** Unlike Mini-Batch K-Means (which updates centroids incrementally via `partial_fit()`), UMAP's manifold is **immutable after training**. This is the fundamental trade-off:

| Approach | Quality | Incremental Update | Reclustering Needed |
|----------|---------|-------------------|---------------------|
| **UMAP + HDBSCAN** | High (0.226 silhouette) | Approximate (transform) | **YES** - eventually |
| **Mini-Batch K-Means** | Lower (0.152 silhouette) | True (partial_fit) | **NO** - never |

**We chose UMAP for quality**, accepting the maintenance burden of periodic reclustering. This story implements that maintenance.

#### Current Limitations (Post-Story 2.3)

| Aspect | Current Behavior | Impact |
|--------|------------------|--------|
| **UMAP Model** | Frozen at initial corpus size (823 chunks) | New semantic spaces not well-represented in 5D projection |
| **Centroids** | Fixed from initial clustering | Don't adapt to new chunks in cluster |
| **Cluster Boundaries** | Never updated | Can't split over-broad clusters or merge similar ones |
| **New Topics** | Forced into existing clusters | Semantically unrelated chunks grouped together |

#### Concrete Example of Drift

**Initial State (Nov 2024):**
```
Cluster 12: "Productivity Systems" (45 chunks)
  - Topics: GTD, Time blocking, Task management
  - Centroid (5D): [1.2, -0.5, 0.3, 2.1, -1.0]
  - Coherence: High (silhouette score: 0.72)
```

**After 6 Months (May 2025):**
```
Cluster 12: "Productivity Systems" (103 chunks)
  - Original topics: GTD, Time blocking, Task management
  - New topics added via delta:
    - Personal knowledge management (Zettelkasten, Obsidian)
    - AI-assisted productivity tools
    - Focus and deep work techniques
  - Centroid (5D): [1.2, -0.5, 0.3, 2.1, -1.0]  âŒ UNCHANGED
  - Coherence: Low (silhouette score: 0.43)  âŒ DEGRADED

Problem: Should be 2-3 distinct clusters, but remains as one!
```

---

## Strategic Assessment: Is Automation Needed Now?

### Current kx-hub Context (as of Nov 2025)

**Data characteristics:**
- Current corpus: 835 chunks
- Growth rate: ~50-100 chunks/month (estimated)
- Content domain: Stable (tech articles, development knowledge)

**Time to trigger thresholds:**
| Trigger | Threshold | Time to Reach |
|---------|-----------|---------------|
| Quarterly | 90 days | **3 months** |
| Absolute growth | +1500 chunks | **15-30 months** (at 50-100/month) |
| Relative growth | 180%+ (â†’2338 chunks) | **15-30 months** |
| Quality degradation | >20% drop | Unknown (gradual) |

### Recommendation: Start Simple

**For current kx-hub scale, the automated system is likely over-engineered.** Consider a phased approach:

#### **Phase 0: Manual Reclustering (Recommended for Year 1)**

```bash
# Run quarterly (every 3 months)
python3 -m src.clustering.initial_load

# Duration: ~10 seconds
# Cost: ~$0.01 per run
# Effort: 5 minutes per quarter (including validation)
```

**Benefits:**
- Zero infrastructure complexity
- Immediate value from reclustering
- Time to learn optimal frequency
- Deferred engineering cost (~2-3 weeks)

**Triggers for moving to Phase 1:**
- Corpus reaches 5,000+ chunks
- Growth rate exceeds 200 chunks/month
- Manual reclustering becomes burdensome
- Quality issues occur between manual runs

#### **Phase 1: Automated Periodic Reclustering (This Story)**

Implement the full automated system with drift detection when:
- Manual process breaks down at scale
- Have operational data on actual drift patterns
- Can justify 2-3 weeks of engineering effort

---

## Solution Architecture (Full Automation)

### Core Principle: Automated Cluster Evolution

Implement **automated periodic re-clustering** that:
1. Detects when cluster quality has degraded
2. Triggers full re-clustering on updated corpus
3. Maps old cluster IDs to new cluster IDs for continuity
4. Updates UMAP model and centroids
5. Preserves cluster names/metadata where possible

---

## System Design

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PERIODIC RE-CLUSTERING SYSTEM                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  1. DRIFT DETECTION (Weekly Cloud Scheduler)         â”‚     â”‚
â”‚  â”‚     - Monitor cluster quality metrics                â”‚     â”‚
â”‚  â”‚     - Track corpus growth rate                       â”‚     â”‚
â”‚  â”‚     - Check days since last re-clustering            â”‚     â”‚
â”‚  â”‚     - Manual override flag                           â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â”‚ Trigger if needed                   â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  2. RE-CLUSTERING JOB (Cloud Run)                    â”‚     â”‚
â”‚  â”‚     - Load full corpus (all chunks)                  â”‚     â”‚
â”‚  â”‚     - Re-train UMAP on full dataset                  â”‚     â”‚
â”‚  â”‚     - Re-run HDBSCAN clustering                      â”‚     â”‚
â”‚  â”‚     - Map old cluster IDs â†’ new cluster IDs          â”‚     â”‚
â”‚  â”‚     - Update all kb_items in Firestore               â”‚     â”‚
â”‚  â”‚     - Regenerate cluster metadata (if needed)        â”‚     â”‚
â”‚  â”‚     - Save new UMAP model to Cloud Storage           â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  3. DELTA CLUSTERING (Resumes with new model)        â”‚     â”‚
â”‚  â”‚     - Loads updated UMAP model                       â”‚     â”‚
â”‚  â”‚     - Uses updated 5D centroids                      â”‚     â”‚
â”‚  â”‚     - Cluster quality maintained                     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Technical Implementation

### 1. Drift Detection Triggers

#### Firestore Metadata Schema

```javascript
// Collection: clustering_metadata
// Document: current_state
{
  "last_reclustering_date": "2025-11-02T10:30:00Z",
  "last_reclustering_size": 823,              // Corpus size at last re-clustering
  "last_reclustering_cluster_count": 38,
  "last_umap_version": "umap_v1_20251102",
  "current_corpus_size": 1247,                // Updated daily
  "avg_silhouette_score": 0.68,               // Cluster quality metric
  "force_reclustering": false,                // Manual override
  "reclustering_in_progress": false,
  "updated_at": "2025-11-02T15:45:00Z"
}
```

#### Trigger Logic

**Based on empirical analysis** (see [clustering-reclustering-strategy.md](../clustering-reclustering-strategy.md)):

```python
def should_trigger_reclustering(db: firestore.Client) -> tuple[bool, str]:
    """
    Determine if re-clustering should be triggered.

    Triggers (refined based on kx-hub analysis):
    1. Manual override flag set to True
    2. 90+ days since last re-clustering (quarterly maintenance)
    3. Absolute growth: +1500 chunks since last re-clustering (corpus size threshold)
    4. Relative growth: 180%+ corpus growth (data distribution changed significantly)
    5. Average silhouette score drops >20% from baseline

    Rationale:
    - UMAP transform works well for moderate delta (up to ~1500 new chunks)
    - Quality degradation is gradual, not catastrophic
    - Quarterly maintenance prevents drift accumulation
    - Growth triggers catch rapid expansion scenarios

    Returns:
        (should_trigger, reason)
    """
    metadata = db.collection('clustering_metadata').document('current_state').get()
    data = metadata.to_dict()

    # Manual override
    if data.get('force_reclustering', False):
        return True, "Manual override flag set"

    # Time-based trigger (quarterly maintenance)
    last_date = datetime.fromisoformat(data['last_reclustering_date'].replace('Z', '+00:00'))
    days_since = (datetime.now(timezone.utc) - last_date).days

    if days_since >= 90:
        return True, f"Quarterly maintenance due ({days_since} days since last reclustering)"

    # Absolute growth trigger (corpus size threshold)
    last_size = data['last_reclustering_size']
    current_size = data['current_corpus_size']
    absolute_growth = current_size - last_size

    if absolute_growth >= 1500:
        return True, f"Corpus grew by 1500+ chunks ({absolute_growth} new chunks)"

    # Relative growth trigger (data distribution changed significantly)
    if last_size > 0:
        growth_rate = absolute_growth / last_size

        if growth_rate >= 1.80:  # 180% growth (e.g., 835 â†’ 2338)
            return True, f"Major corpus expansion ({growth_rate:.1%} growth)"

    # Quality-based trigger (silhouette score degradation)
    baseline_score = data.get('baseline_silhouette_score', 0.23)  # From initial clustering
    current_score = data.get('avg_silhouette_score', baseline_score)

    if baseline_score > 0:
        degradation = (baseline_score - current_score) / baseline_score

        if degradation > 0.20:  # >20% degradation
            return True, f"Cluster quality degraded by {degradation:.1%} (score: {current_score:.3f})"

    return False, "No trigger conditions met - UMAP transform still effective"
```

#### Cloud Scheduler (Weekly Check)

```python
# functions/clustering/drift_detector.py

@functions_framework.http
def check_reclustering_trigger(request):
    """
    Weekly Cloud Function to check if re-clustering is needed.

    Triggered by: Cloud Scheduler (every Sunday at 2am UTC)
    """
    logger.info("Checking re-clustering triggers...")

    project_id = os.getenv('GCP_PROJECT')
    db = firestore.Client(project=project_id)

    # Check if re-clustering already in progress
    metadata = db.collection('clustering_metadata').document('current_state').get()
    if metadata.get('reclustering_in_progress', False):
        logger.info("Re-clustering already in progress, skipping")
        return {'status': 'in_progress'}, 200

    # Check triggers
    should_trigger, reason = should_trigger_reclustering(db)

    if should_trigger:
        logger.info(f"Triggering re-clustering: {reason}")

        # Set in-progress flag
        db.collection('clustering_metadata').document('current_state').update({
            'reclustering_in_progress': True,
            'reclustering_trigger_reason': reason,
            'reclustering_started_at': datetime.utcnow().isoformat() + 'Z'
        })

        # Trigger Cloud Run job
        trigger_cloud_run_job('reclustering-job', {'reason': reason})

        return {
            'status': 'triggered',
            'reason': reason
        }, 200
    else:
        logger.info(f"Re-clustering not needed: {reason}")
        return {
            'status': 'not_needed',
            'reason': reason
        }, 200
```

---

### 2. Re-clustering Cloud Run Job

#### Why Cloud Run (not Cloud Function)

**Cloud Functions limitations:**
- Max execution time: 9 minutes (3rd gen) / 60 minutes (2nd gen)
- Limited memory: 8GB max
- Cold starts

**Cloud Run advantages:**
- Max execution time: 60 minutes
- Memory: Up to 32GB
- More suitable for batch processing
- Can run complex ML operations (UMAP re-training)

#### Job Implementation

```python
# cloud_run/reclustering_job/main.py

import logging
import time
from typing import Dict, Any, List
import numpy as np
from google.cloud import firestore
from google.cloud import storage
import pickle

# Import from shared module
from clustering.clusterer import SemanticClusterer
from clustering.cluster_metadata import ClusterMetadataGenerator

logger = logging.getLogger(__name__)


class ReclusteringJob:
    """
    Handles full corpus re-clustering with cluster ID mapping.
    """

    def __init__(self, project_id: str, reason: str):
        self.project_id = project_id
        self.reason = reason
        self.db = firestore.Client(project=project_id)
        self.storage_client = storage.Client(project=project_id)

    def run(self) -> Dict[str, Any]:
        """
        Execute full re-clustering workflow.

        Returns:
            Summary statistics
        """
        logger.info("=" * 70)
        logger.info("AUTOMATED RE-CLUSTERING JOB - START")
        logger.info(f"Trigger reason: {self.reason}")
        logger.info("=" * 70)

        start_time = time.time()

        try:
            # Step 1: Load old cluster assignments (for mapping)
            logger.info("\n[Step 1/8] Loading old cluster assignments...")
            old_assignments = self._load_old_assignments()

            # Step 2: Load full corpus
            logger.info("\n[Step 2/8] Loading full corpus...")
            chunks, embeddings, chunk_ids = self._load_full_corpus()
            logger.info(f"Loaded {len(chunks)} chunks with embeddings")

            # Step 3: Re-train UMAP on full corpus
            logger.info("\n[Step 3/8] Re-training UMAP on full corpus...")
            clusterer = SemanticClusterer(
                algorithm='hdbscan',
                min_cluster_size=10,
                min_samples=3,
                use_umap=True,
                umap_n_components=5
            )
            new_labels = clusterer.fit_predict(embeddings)
            logger.info(f"Clustering complete: {clusterer.n_clusters_found} clusters found")

            # Step 4: Map old cluster IDs to new cluster IDs
            logger.info("\n[Step 4/8] Mapping old clusters to new clusters...")
            cluster_mapping = self._map_old_to_new_clusters(
                old_assignments, new_labels, chunk_ids
            )

            # Step 5: Update Firestore with new assignments
            logger.info("\n[Step 5/8] Updating Firestore with new cluster assignments...")
            self._update_firestore_assignments(chunk_ids, new_labels, cluster_mapping)

            # Step 6: Save new UMAP model
            logger.info("\n[Step 6/8] Saving new UMAP model to Cloud Storage...")
            umap_version = self._save_umap_model(clusterer.umap_model)

            # Step 7: Regenerate cluster metadata
            logger.info("\n[Step 7/8] Regenerating cluster metadata...")
            self._regenerate_cluster_metadata(cluster_mapping)

            # Step 8: Update clustering metadata
            logger.info("\n[Step 8/8] Updating clustering metadata...")
            elapsed = time.time() - start_time
            self._update_metadata(len(chunks), clusterer.n_clusters_found, umap_version)

            logger.info("\n" + "=" * 70)
            logger.info("AUTOMATED RE-CLUSTERING JOB - COMPLETE")
            logger.info(f"Duration: {elapsed:.1f} seconds")
            logger.info("=" * 70)

            return {
                'status': 'success',
                'chunks_reclustered': len(chunks),
                'old_cluster_count': len(set(old_assignments.values())),
                'new_cluster_count': clusterer.n_clusters_found,
                'clusters_preserved': sum(1 for v in cluster_mapping.values() if v != 'new'),
                'duration_seconds': round(elapsed, 1),
                'umap_version': umap_version
            }

        except Exception as e:
            logger.error(f"Re-clustering failed: {e}", exc_info=True)

            # Clear in-progress flag
            self.db.collection('clustering_metadata').document('current_state').update({
                'reclustering_in_progress': False,
                'last_reclustering_error': str(e),
                'last_reclustering_error_at': datetime.utcnow().isoformat() + 'Z'
            })

            raise

    def _load_old_assignments(self) -> Dict[str, str]:
        """
        Load current cluster assignments before re-clustering.

        Returns:
            Dict mapping chunk_id â†’ cluster_id
        """
        old_assignments = {}

        docs = self.db.collection('kb_items').stream()
        for doc in docs:
            data = doc.to_dict()
            cluster_ids = data.get('cluster_id', [])
            if cluster_ids:
                old_assignments[doc.id] = cluster_ids[0]  # Primary cluster

        return old_assignments

    def _load_full_corpus(self) -> tuple[List[Dict], np.ndarray, List[str]]:
        """
        Load all chunks with embeddings from Firestore.

        Returns:
            (chunks, embeddings_array, chunk_ids)
        """
        chunks = []
        embeddings = []
        chunk_ids = []

        docs = self.db.collection('kb_items').stream()

        for doc in docs:
            data = doc.to_dict()

            # Check for embedding
            if 'embedding' not in data:
                continue

            # Extract embedding
            embedding = data['embedding']
            if hasattr(embedding, 'to_map_value'):
                map_value = embedding.to_map_value()
                embedding_array = np.array(map_value.get('value', map_value), dtype=np.float32)
            elif isinstance(embedding, list):
                embedding_array = np.array(embedding, dtype=np.float32)
            else:
                continue

            if len(embedding_array) != 768:
                continue

            chunks.append(data)
            embeddings.append(embedding_array)
            chunk_ids.append(doc.id)

        return chunks, np.array(embeddings), chunk_ids

    def _map_old_to_new_clusters(
        self,
        old_assignments: Dict[str, str],
        new_labels: np.ndarray,
        chunk_ids: List[str]
    ) -> Dict[int, str]:
        """
        Map new cluster labels to old cluster IDs based on overlap.

        Strategy:
        - If new cluster has >70% overlap with old cluster, preserve old ID
        - Otherwise, create new cluster ID

        Returns:
            Dict mapping new_label â†’ old_cluster_id or 'new'
        """
        mapping = {}

        unique_new_labels = np.unique(new_labels)

        for new_label in unique_new_labels:
            if new_label == -1:  # Noise cluster
                mapping[new_label] = 'noise'
                continue

            # Get chunks in this new cluster
            indices = np.where(new_labels == new_label)[0]
            chunks_in_new = [chunk_ids[i] for i in indices]

            # Count overlap with each old cluster
            old_cluster_overlap = {}
            for chunk_id in chunks_in_new:
                old_cluster = old_assignments.get(chunk_id)
                if old_cluster:
                    old_cluster_overlap[old_cluster] = old_cluster_overlap.get(old_cluster, 0) + 1

            if not old_cluster_overlap:
                # Completely new cluster
                mapping[new_label] = 'new'
                continue

            # Find old cluster with highest overlap
            best_old_cluster = max(old_cluster_overlap, key=old_cluster_overlap.get)
            overlap_count = old_cluster_overlap[best_old_cluster]
            overlap_ratio = overlap_count / len(chunks_in_new)

            if overlap_ratio >= 0.70:
                # Preserve old cluster ID
                mapping[new_label] = best_old_cluster
                logger.info(f"  New cluster {new_label} â†’ {best_old_cluster} ({overlap_ratio:.1%} overlap)")
            else:
                # Create new cluster ID
                mapping[new_label] = 'new'
                logger.info(f"  New cluster {new_label} â†’ NEW cluster ({overlap_ratio:.1%} best overlap)")

        return mapping

    def _update_firestore_assignments(
        self,
        chunk_ids: List[str],
        new_labels: np.ndarray,
        cluster_mapping: Dict[int, str]
    ):
        """
        Update all kb_items with new cluster assignments.
        """
        # Generate new cluster IDs for 'new' clusters
        existing_cluster_ids = set(cluster_mapping.values()) - {'new', 'noise'}
        next_cluster_num = max([int(cid.split('-')[1]) for cid in existing_cluster_ids if cid.startswith('cluster-')], default=0) + 1

        new_cluster_assignments = {}

        batch = self.db.batch()
        write_count = 0

        for i, chunk_id in enumerate(chunk_ids):
            new_label = new_labels[i]
            mapped_cluster = cluster_mapping[new_label]

            # Assign cluster ID
            if mapped_cluster == 'noise':
                cluster_id = 'noise'
            elif mapped_cluster == 'new':
                # Generate new cluster ID
                if new_label not in new_cluster_assignments:
                    new_cluster_assignments[new_label] = f'cluster-{next_cluster_num}'
                    next_cluster_num += 1
                cluster_id = new_cluster_assignments[new_label]
            else:
                cluster_id = mapped_cluster

            # Update document
            doc_ref = self.db.collection('kb_items').document(chunk_id)
            batch.update(doc_ref, {'cluster_id': [cluster_id]})
            write_count += 1

            # Commit every 500 operations
            if write_count == 500:
                batch.commit()
                batch = self.db.batch()
                write_count = 0

        # Commit remaining
        if write_count > 0:
            batch.commit()

        logger.info(f"Updated {len(chunk_ids)} chunks with new cluster assignments")

    def _save_umap_model(self, umap_model) -> str:
        """
        Save UMAP model to Cloud Storage with version.

        Returns:
            Version string (e.g., 'umap_v2_20251102')
        """
        from datetime import datetime

        version = f"umap_v2_{datetime.utcnow().strftime('%Y%m%d')}"

        # Pickle UMAP model
        model_bytes = pickle.dumps(umap_model)

        # Upload to Cloud Storage
        bucket = self.storage_client.bucket(f'{self.project_id}-pipeline')
        blob = bucket.blob(f'models/{version}.pkl')
        blob.upload_from_string(model_bytes)

        logger.info(f"Saved UMAP model: gs://{bucket.name}/models/{version}.pkl")

        return version

    def _regenerate_cluster_metadata(self, cluster_mapping: Dict[int, str]):
        """
        Regenerate cluster metadata (names, descriptions, centroids).

        For preserved clusters: Keep existing names
        For new clusters: Generate new names via Gemini
        """
        # Use ClusterMetadataGenerator
        metadata_gen = ClusterMetadataGenerator(
            project_id=self.project_id,
            region='europe-west4',
            kb_collection='kb_items',
            clusters_collection='clusters'
        )

        # Generate metadata for all clusters
        metadata_gen.generate_all_clusters()

    def _update_metadata(self, corpus_size: int, cluster_count: int, umap_version: str):
        """
        Update clustering_metadata with new state.
        """
        self.db.collection('clustering_metadata').document('current_state').update({
            'last_reclustering_date': datetime.utcnow().isoformat() + 'Z',
            'last_reclustering_size': corpus_size,
            'last_reclustering_cluster_count': cluster_count,
            'last_umap_version': umap_version,
            'current_corpus_size': corpus_size,
            'reclustering_in_progress': False,
            'force_reclustering': False,
            'updated_at': datetime.utcnow().isoformat() + 'Z'
        })


# Flask app for Cloud Run
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/run', methods=['POST'])
def run_reclustering():
    """
    Cloud Run endpoint for re-clustering job.
    """
    request_json = request.get_json(silent=True)
    reason = request_json.get('reason', 'Manual trigger') if request_json else 'Manual trigger'

    project_id = os.getenv('GCP_PROJECT')

    job = ReclusteringJob(project_id, reason)
    result = job.run()

    return jsonify(result), 200


if __name__ == '__main__':
    port = int(os.getenv('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
```

---

### 3. Infrastructure Setup

#### Cloud Scheduler Configuration

```yaml
# terraform/cloud_scheduler.tf

resource "google_cloud_scheduler_job" "reclustering_check" {
  name             = "reclustering-drift-check"
  description      = "Weekly check for cluster drift triggers"
  schedule         = "0 2 * * 0"  # Every Sunday at 2am UTC
  time_zone        = "UTC"
  attempt_deadline = "320s"

  http_target {
    http_method = "POST"
    uri         = "https://europe-west4-${var.project_id}.cloudfunctions.net/drift-detector"

    oidc_token {
      service_account_email = google_service_account.pipeline.email
    }
  }
}
```

#### Cloud Run Job Configuration

```yaml
# terraform/cloud_run.tf

resource "google_cloud_run_v2_job" "reclustering" {
  name     = "reclustering-job"
  location = var.region

  template {
    template {
      containers {
        image = "gcr.io/${var.project_id}/reclustering-job:latest"

        resources {
          limits = {
            cpu    = "4"
            memory = "16Gi"
          }
        }

        env {
          name  = "GCP_PROJECT"
          value = var.project_id
        }
      }

      timeout         = "3600s"  # 60 minutes max
      max_retries     = 1
      service_account = google_service_account.pipeline.email
    }
  }
}
```

---

## Performance & Cost Analysis

### Expected Performance

| Corpus Size | UMAP Training | HDBSCAN | Firestore Updates | Total Time |
|-------------|---------------|---------|-------------------|------------|
| 1,000 chunks | ~5 seconds | ~3 seconds | ~2 seconds | **~10 seconds** |
| 5,000 chunks | ~25 seconds | ~15 seconds | ~10 seconds | **~50 seconds** |
| 10,000 chunks | ~60 seconds | ~40 seconds | ~20 seconds | **~2 minutes** |
| 50,000 chunks | ~5 minutes | ~4 minutes | ~2 minutes | **~11 minutes** |

### Cost Analysis

#### Per Re-clustering Cost

| Component | Cost | Frequency | Monthly Cost |
|-----------|------|-----------|--------------|
| Cloud Run (CPU) | $0.10 per re-cluster | Quarterly | **$0.03/month** |
| Firestore reads | $0.01 | Quarterly | **$0.003/month** |
| Firestore writes | $0.02 | Quarterly | **$0.007/month** |
| Cloud Storage (UMAP model) | $0.001 | One-time | **$0.001/month** |
| Gemini API (cluster names) | $0.10 | Only for new clusters | **~$0.03/month** |
| **Total** | | | **~$0.07/month** |

#### Annual Cost: **~$0.84/year**

**Note:** Cost scales with corpus size, but remains <$5/year even at 50,000 chunks.

---

## Acceptance Criteria

### Phase 0: Manual Reclustering (CURRENT - Story 2.3 Complete)

- [x] Manual reclustering works via `python3 -m src.clustering.initial_load`
- [x] UMAP model saved to Cloud Storage
- [x] Centroids updated in Firestore
- [x] Delta clustering picks up new UMAP model automatically
- [x] Documentation exists ([clustering-reclustering-strategy.md](../clustering-reclustering-strategy.md))

**Recommendation:** Stay in Phase 0 until corpus reaches 5,000+ chunks or growth rate exceeds 200/month.

### Phase 1: Automated System (THIS STORY - Implement When Needed)

**Infrastructure:**
- [ ] Drift detection Cloud Function deployed
- [ ] Weekly Cloud Scheduler job configured (initially can be monthly)
- [ ] Re-clustering Cloud Run job deployed and tested
- [ ] Firestore metadata schema created (`clustering_metadata` collection)

**Quality & Correctness:**
- [ ] Cluster ID mapping preserves >90% of old cluster IDs (70%+ overlap threshold)
- [ ] UMAP model versioning implemented (e.g., `umap_v2_20251102`)
- [ ] Re-clustering completes in <5 minutes for 5,000 chunks
- [ ] Delta clustering automatically picks up new UMAP model post-recluster

**Monitoring & Operations:**
- [ ] Manual trigger mechanism works (`force_reclustering` flag)
- [ ] Cluster quality metrics logged after each re-clustering
- [ ] Alerts configured for re-clustering failures
- [ ] Cloud Monitoring dashboard shows drift metrics

**Documentation:**
- [ ] Runbook for manual reclustering (Phase 0) âœ… (exists)
- [ ] Runbook for automated system troubleshooting
- [ ] Trigger threshold tuning guide
- [ ] Migration guide from Phase 0 to Phase 1

---

## Testing Strategy

### Unit Tests

```python
def test_drift_detection_triggers():
    """Test that triggers fire correctly."""
    # Time-based trigger
    assert should_trigger_reclustering(mock_db(days_since=95)) == (True, "90+ days...")

    # Growth-based trigger
    assert should_trigger_reclustering(mock_db(growth=0.25)) == (True, "20%+ corpus...")

    # Quality-based trigger
    assert should_trigger_reclustering(mock_db(silhouette=0.52)) == (True, "Cluster quality...")

    # No trigger
    assert should_trigger_reclustering(mock_db(days_since=30, growth=0.05)) == (False, "No trigger...")

def test_cluster_mapping_preserves_ids():
    """Test cluster ID mapping logic."""
    old_assignments = {'chunk1': 'cluster-5', 'chunk2': 'cluster-5', 'chunk3': 'cluster-8'}
    new_labels = np.array([0, 0, 1])
    chunk_ids = ['chunk1', 'chunk2', 'chunk3']

    mapping = map_old_to_new_clusters(old_assignments, new_labels, chunk_ids)

    # New cluster 0 should map to old cluster-5 (100% overlap)
    assert mapping[0] == 'cluster-5'
```

### Integration Tests

```python
def test_full_reclustering_workflow():
    """End-to-end test of re-clustering."""
    # 1. Setup: Create test corpus with known clusters
    # 2. Run initial clustering
    # 3. Add new chunks
    # 4. Trigger re-clustering
    # 5. Verify cluster IDs mostly preserved
    # 6. Verify new UMAP model saved
    # 7. Verify delta clustering uses new model
```

---

## Migration Plan

### Phase 1: Deploy Infrastructure (Week 1)

1. Deploy drift detector Cloud Function
2. Configure Cloud Scheduler
3. Deploy re-clustering Cloud Run job
4. Create Firestore metadata schema

### Phase 2: Initial Baseline (Week 1)

1. Run re-clustering manually once to establish baseline
2. Verify cluster ID mapping works
3. Set initial metadata values

### Phase 3: Monitoring (Week 2-4)

1. Monitor drift metrics weekly
2. Tune trigger thresholds based on observed drift
3. Adjust re-clustering frequency if needed

---

## Monitoring & Alerts

### Cloud Monitoring Metrics

```python
# Log metrics after each re-clustering
from google.cloud import monitoring_v3

def log_reclustering_metrics(result: Dict):
    """
    Log custom metrics to Cloud Monitoring.
    """
    client = monitoring_v3.MetricServiceClient()
    project_name = f"projects/{project_id}"

    # Cluster count metric
    series = monitoring_v3.TimeSeries()
    series.metric.type = 'custom.googleapis.com/clustering/cluster_count'
    point = monitoring_v3.Point()
    point.value.int64_value = result['new_cluster_count']
    point.interval.end_time.seconds = int(time.time())
    series.points = [point]

    client.create_time_series(name=project_name, time_series=[series])
```

### Recommended Alerts

1. **Re-clustering failure:** Alert if re-clustering job fails
2. **Long re-clustering duration:** Alert if re-clustering takes >10 minutes
3. **Low cluster preservation:** Alert if <70% of clusters preserved
4. **Drift detection disabled:** Alert if weekly check hasn't run in 10 days

---

## Risks and Mitigations

### Risk 1: Re-clustering During Active Usage

**Risk:** Re-clustering updates cluster IDs while users are browsing/searching

**Mitigation:**
- Run re-clustering during low-usage window (Sundays 2am UTC)
- Use Firestore transactions for atomic updates
- MCP server caches cluster metadata (eventual consistency acceptable)

### Risk 2: Cluster ID Churn

**Risk:** Even with 70% overlap threshold, some clusters change IDs frequently

**Mitigation:**
- Track cluster ID changes over time
- If cluster changes ID >3 times in 6 months, manually stabilize
- Consider increasing overlap threshold to 80%

### Risk 3: UMAP Non-determinism

**Risk:** UMAP with same random_state can produce slightly different results on different runs

**Mitigation:**
- Accept minor variance as acceptable (clusters will be semantically similar)
- Use cluster ID mapping to preserve continuity
- Monitor cluster quality metrics (silhouette score) to detect degradation

---

## Future Enhancements

### 1. Incremental Centroid Updates (Between Re-clusterings)

Add lightweight centroid updates during delta clustering:

```python
# After assigning new chunk to cluster
def update_centroid_incremental(cluster_id, new_embedding_5d, current_size):
    cluster_doc = db.collection('clusters').document(cluster_id)
    current_centroid = cluster_doc.get().get('centroid_5d')

    updated_centroid = (current_centroid * current_size + new_embedding_5d) / (current_size + 1)

    cluster_doc.update({
        'centroid_5d': updated_centroid,
        'size': current_size + 1
    })
```

**Benefit:** Reduces drift between re-clusterings

### 2. A/B Testing for Cluster Quality

Compare cluster quality before/after re-clustering:

```python
def evaluate_cluster_quality_improvement(old_labels, new_labels, embeddings):
    """
    Measure if re-clustering improved cluster quality.
    """
    old_score = silhouette_score(embeddings, old_labels)
    new_score = silhouette_score(embeddings, new_labels)

    improvement = new_score - old_score

    logger.info(f"Cluster quality: {old_score:.3f} â†’ {new_score:.3f} (Î”{improvement:+.3f})")

    return improvement
```

### 3. Smart Trigger Tuning

Use ML to predict optimal re-clustering frequency:

```python
# Track drift rate over time
# Predict when silhouette score will drop below 0.55
# Trigger re-clustering proactively before quality degrades
```

---

## References

- [UMAP Documentation: Transforming New Data](https://umap-learn.readthedocs.io/en/latest/transform.html)
- [HDBSCAN: Hierarchical Density-Based Clustering](https://hdbscan.readthedocs.io/)
- [Cloud Run Jobs](https://cloud.google.com/run/docs/create-jobs)
- [Silhouette Score for Cluster Quality](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)

---

## Summary & Decision Points

### Current State (Post-Story 2.3)

âœ… **Phase 0 is COMPLETE and SUFFICIENT for current scale:**
- Manual reclustering works (`python3 -m src.clustering.initial_load`)
- Strategic guidance documented
- Quarterly manual process requires ~5 minutes per quarter
- Cost: ~$0.04/year (4 runs Ã— $0.01)

### When to Implement This Story (Phase 1)

**Implement automated reclustering when ANY of:**
1. **Scale threshold:** Corpus reaches 5,000+ chunks
2. **Growth threshold:** Adding >200 chunks/month consistently
3. **Operational burden:** Manual reclustering becomes error-prone or forgotten
4. **Quality issues:** Drift causes user-visible problems between manual runs

**Current timeline estimate:** 15-30 months before automation needed (based on ~50-100 chunks/month growth)

### Implementation Approach

**If implementing now (not recommended):**
- Estimated effort: **2-3 weeks** (includes testing and monitoring setup)
- Infrastructure: Cloud Scheduler + Cloud Function + Cloud Run Job
- Ongoing cost: ~$0.07/month (very low)

**If deferring (recommended):**
- Action: Add calendar reminder for quarterly reclustering
- Action: Monitor corpus size monthly
- Action: Revisit this story when corpus hits 2,500 chunks (50% to threshold)
- Benefit: Defer 2-3 weeks of engineering effort
- Benefit: Learn actual drift patterns before automating

### Business Value

**When implemented at scale:**
- Maintains cluster quality long-term as KB grows
- Enables growth to 50,000+ chunks without quality degradation
- Reduces operational burden at scale
- Provides data-driven reclustering decisions

**At current scale (835 chunks):**
- Minimal business value (manual process is sufficient)
- Engineering time better spent on other features
- Premature optimization

---

**Story Status:** PLANNED (implement when scale justifies automation)
**Priority:** LOW (defer until Phase 0 breaks down)
**Dependencies:** Story 2.3 (COMPLETE âœ…)
**Next Review:** When corpus reaches 2,500 chunks or in 12 months
